#!/usr/bin/env python3
"""
í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸

í¬ë¡¤ë§ëœ LangGraph ê³µì‹ë¬¸ì„œë¥¼ Vector Store(Chroma)ì™€ BM25 ì¸ë±ìŠ¤ë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.
"""

import json
import os
import logging
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# BM25ìš© ì „ì—­ ì „ì²˜ë¦¬ í•¨ìˆ˜ (pickle í˜¸í™˜ì„±ì„ ìœ„í•´)
def bm25_preprocess_func(text: str) -> List[str]:
    """BM25ìš© ì „ì²˜ë¦¬ í•¨ìˆ˜ (í•œêµ­ì–´ í† í¬ë‚˜ì´ì €)"""
    try:
        from kiwipiepy import Kiwi
        kiwi = Kiwi()
        
        # LangGraph ê´€ë ¨ ì „ë¬¸ ìš©ì–´ ì¶”ê°€
        kiwi.add_user_word('LangGraph', 'NNP')
        kiwi.add_user_word('StateGraph', 'NNP')
        kiwi.add_user_word('MessageGraph', 'NNP')
        kiwi.add_user_word('GraphState', 'NNP')
        kiwi.add_user_word('Checkpointer', 'NNP')
        kiwi.add_user_word('HITL', 'NNP')  # Human-in-the-loop
        
        tokens = kiwi.tokenize(text)
        return [token.form for token in tokens if len(token.form) > 1]
    except Exception:
        # fallback: ê¸°ë³¸ ë¶„í• 
        return text.lower().split()

def load_crawled_docs(jsonl_file: str) -> List[Dict[str, Any]]:
    """í¬ë¡¤ë§ëœ ë¬¸ì„œë“¤ ë¡œë“œ"""
    docs = []
    try:
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in f:
                doc = json.loads(line.strip())
                docs.append(doc)
        
        logger.info(f"ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(docs)}ê°œ")
        return docs
    
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

def create_document_chunks(docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ë¬¸ì„œë¥¼ ì²­í‚¹í•˜ì—¬ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë‹¨ìœ„ë¡œ ë¶„í• """
    try:
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        from langchain_core.documents import Document
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,        # ì²­í¬ í¬ê¸°
            chunk_overlap=50,      # ì²­í¬ ê²¹ì¹¨
            length_function=len,
            separators=["\n\n", "\n", "## ", ". ", " ", ""]
        )
        
        all_chunks = []
        
        for doc_idx, doc in enumerate(docs):
            content = doc['content']
            title = doc['title']
            url = doc['url']
            
            # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
            chunks = text_splitter.split_text(content)
            
            for chunk_idx, chunk in enumerate(chunks):
                if len(chunk.strip()) < 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ëŠ” ì œì™¸
                    continue
                
                # Document ê°ì²´ ìƒì„±
                chunk_doc = {
                    "page_content": chunk.strip(),
                    "metadata": {
                        "source": url,
                        "title": title,
                        "chunk_index": chunk_idx,
                        "doc_index": doc_idx,
                        "content_length": len(chunk),
                        "doc_type": "langgraph_official_doc",
                        "crawled_at": doc.get('crawled_at', ''),
                    }
                }
                all_chunks.append(chunk_doc)
        
        logger.info(f"ì²­í‚¹ ì™„ë£Œ: {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
        return all_chunks
        
    except Exception as e:
        logger.error(f"ì²­í‚¹ ì‹¤íŒ¨: {e}")
        return []

def build_vector_store(chunks: List[Dict[str, Any]]) -> str:
    """Vector Store (Chroma) êµ¬ì¶•"""
    try:
        from langchain_chroma import Chroma
        from langchain_openai import OpenAIEmbeddings
        from langchain_google_genai import GoogleGenerativeAIEmbeddings
        from langchain_core.documents import Document
        
        # Document ê°ì²´ë¡œ ë³€í™˜
        documents = []
        for chunk in chunks:
            doc = Document(
                page_content=chunk["page_content"],
                metadata=chunk["metadata"]
            )
            documents.append(doc)
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embeddings = None
        try:
            embeddings = OpenAIEmbeddings(
                model="text-embedding-3-small",
                openai_api_key=os.getenv("OPENAI_API_KEY")
            )
            logger.info("OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.warning(f"OpenAI ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {e}, Google ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´")
            try:
                embeddings = GoogleGenerativeAIEmbeddings(
                    model="models/embedding-001",
                    google_api_key=os.getenv("GOOGLE_API_KEY")
                )
                logger.info("Google ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e2:
                logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ì „ ì‹¤íŒ¨: {e2}")
                return None
        
        # Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        persist_directory = "./data/chroma_db_langgraph"
        
        # ê¸°ì¡´ DB ì‚­ì œ (ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì¬êµ¬ì¶•)
        if os.path.exists(persist_directory):
            import shutil
            shutil.rmtree(persist_directory)
            logger.info("ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ ì™„ë£Œ")
        
        # ìƒˆ ë²¡í„° DB ìƒì„±
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            collection_name="langgraph_docs",
            persist_directory=persist_directory,
            collection_metadata={"hnsw:space": "cosine"}
        )
        
        logger.info(f"Vector Store ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        logger.info(f"ì €ì¥ ìœ„ì¹˜: {persist_directory}")
        
        return persist_directory
        
    except Exception as e:
        logger.error(f"Vector Store êµ¬ì¶• ì‹¤íŒ¨: {e}")
        return None

def build_bm25_index(chunks: List[Dict[str, Any]]) -> str:
    """BM25 ì¸ë±ìŠ¤ êµ¬ì¶• (í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í¬í•¨)"""
    try:
        from langchain_community.retrievers import BM25Retriever
        from langchain_core.documents import Document
        import pickle
        
        # Document ê°ì²´ë¡œ ë³€í™˜
        documents = []
        for chunk in chunks:
            doc = Document(
                page_content=chunk["page_content"],
                metadata=chunk["metadata"]
            )
            documents.append(doc)
        
        # BM25 Retriever ìƒì„±
        bm25_retriever = BM25Retriever.from_documents(
            documents=documents,
            preprocess_func=bm25_preprocess_func
        )
        
        # BM25 ì¸ë±ìŠ¤ ì €ì¥ (ë¬¸ì„œë“¤ë§Œ ì €ì¥, retrieverëŠ” ì¬ìƒì„±)
        bm25_dir = Path("./data/bm25_index_langgraph")
        bm25_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¬¸ì„œë“¤ì„ JSONìœ¼ë¡œ ì €ì¥
        docs_file = bm25_dir / "documents.json"
        docs_data = []
        for doc in documents:
            docs_data.append({
                "page_content": doc.page_content,
                "metadata": doc.metadata
            })
        
        with open(docs_file, 'w', encoding='utf-8') as f:
            json.dump(docs_data, f, ensure_ascii=False, indent=2)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "total_documents": len(documents),
            "preprocess_func": "bm25_preprocess_func",
            "created_at": "2025-07-18",
            "source": "LangGraph ê³µì‹ë¬¸ì„œ"
        }
        
        metadata_file = bm25_dir / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        logger.info(f"ì €ì¥ ìœ„ì¹˜: {docs_file}")
        
        return str(docs_file)
        
    except Exception as e:
        logger.error(f"BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
        return None

def test_hybrid_search(vector_db_path: str, bm25_path: str):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    try:
        from langchain_chroma import Chroma
        from langchain_openai import OpenAIEmbeddings
        from langchain_google_genai import GoogleGenerativeAIEmbeddings
        from langchain.retrievers import EnsembleRetriever
        from langchain_community.retrievers import BM25Retriever
        from langchain_core.documents import Document
        
        logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # Vector Store ë¡œë“œ
        embeddings = None
        try:
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        except Exception:
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        
        vectorstore = Chroma(
            collection_name="langgraph_docs",
            embedding_function=embeddings,
            persist_directory=vector_db_path
        )
        
        vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        
        # BM25 Retriever ì¬ìƒì„± (JSONì—ì„œ ë¬¸ì„œ ë¡œë“œ)
        with open(bm25_path, 'r', encoding='utf-8') as f:
            docs_data = json.load(f)
        
        documents = []
        for doc_data in docs_data:
            doc = Document(
                page_content=doc_data["page_content"],
                metadata=doc_data["metadata"]
            )
            documents.append(doc)
        
        bm25_retriever = BM25Retriever.from_documents(
            documents=documents,
            preprocess_func=bm25_preprocess_func
        )
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸° ìƒì„±
        ensemble_retriever = EnsembleRetriever(
            retrievers=[vector_retriever, bm25_retriever],
            weights=[0.6, 0.4]  # Vector: 60%, BM25: 40%
        )
        
        # í…ŒìŠ¤íŠ¸ ì§ˆì˜
        test_queries = [
            "LangGraph StateGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ë§Œë“¤ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
            "persistenceì™€ checkpointingì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "human in the loop ê¸°ëŠ¥ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?"
        ]
        
        for query in test_queries:
            logger.info(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì§ˆì˜: {query}")
            
            try:
                results = ensemble_retriever.invoke(query)
                logger.info(f"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                
                for i, doc in enumerate(results[:2]):  # ìƒìœ„ 2ê°œë§Œ ì¶œë ¥
                    logger.info(f"[{i+1}] ì œëª©: {doc.metadata.get('title', 'N/A')}")
                    logger.info(f"    ë‚´ìš©: {doc.page_content[:100]}...")
                    
            except Exception as e:
                logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        logger.info("âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘")
    print("=" * 50)
    
    # 1. í¬ë¡¤ë§ëœ ë¬¸ì„œ ë¡œë“œ
    jsonl_file = "data/langgraph_docs/langgraph_docs.jsonl"
    if not os.path.exists(jsonl_file):
        print(f"âŒ í¬ë¡¤ë§ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_file}")
        print("ë¨¼ì € scripts/langgraph_docs_crawler.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    docs = load_crawled_docs(jsonl_file)
    if not docs:
        print("âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    # 2. ë¬¸ì„œ ì²­í‚¹
    print("\nğŸ“ ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
    chunks = create_document_chunks(docs)
    if not chunks:
        print("âŒ ì²­í‚¹ ì‹¤íŒ¨")
        return
    
    # 3. Vector Store êµ¬ì¶•
    print("\nğŸ”¤ Vector Store êµ¬ì¶• ì¤‘...")
    vector_db_path = build_vector_store(chunks)
    if not vector_db_path:
        print("âŒ Vector Store êµ¬ì¶• ì‹¤íŒ¨")
        return
    
    # 4. BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
    print("\nğŸ” BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
    bm25_path = build_bm25_index(chunks)
    if not bm25_path:
        print("âŒ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
        return
    
    # 5. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¤‘...")
    test_hybrid_search(vector_db_path, bm25_path)
    
    print("\nâœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ ì²­í¬: {len(chunks)}ê°œ")
    print(f"ğŸ”¤ Vector Store: {vector_db_path}")
    print(f"ğŸ” BM25 Index: {bm25_path}")
    print("\nğŸ’¡ ì´ì œ knowledge_system/nodes/searchers.pyì˜ document_searcherë¥¼ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main() 