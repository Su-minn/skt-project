#!/usr/bin/env python3
"""
LangGraph ê³µì‹ë¬¸ì„œ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸

LangGraph ê°€ì´ë“œ í˜ì´ì§€ë“¤ì„ í¬ë¡¤ë§í•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš© ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import os
import time
import json
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from typing import List, Dict, Any
import logging
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LangGraphDocsCrawler:
    """LangGraph ê³µì‹ë¬¸ì„œ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, base_url: str = "https://langchain-ai.github.io/langgraph/"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = Path("data/langgraph_docs")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def get_page_content(self, url: str) -> Dict[str, Any]:
        """ë‹¨ì¼ í˜ì´ì§€ ë‚´ìš© í¬ë¡¤ë§"""
        try:
            logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ (Material MkDocs êµ¬ì¡° ê¸°ë°˜)
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='md-content')
            
            if not main_content:
                logger.warning(f"ë©”ì¸ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
                return None
                
            # ì œëª© ì¶”ì¶œ
            title = ""
            title_elem = soup.find('h1') or soup.find('title')
            if title_elem:
                title = title_elem.get_text().strip()
            
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ í¬í•¨)
            content = ""
            for elem in main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'pre', 'code']):
                text = elem.get_text().strip()
                if text:
                    if elem.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                        content += f"\n\n## {text}\n\n"
                    elif elem.name in ['pre', 'code']:
                        content += f"\n```\n{text}\n```\n"
                    else:
                        content += f"{text}\n\n"
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            parsed_url = urlparse(url)
            
            doc_data = {
                "title": title,
                "content": content.strip(),
                "url": url,
                "path": parsed_url.path,
                "domain": parsed_url.netloc,
                "source": "LangGraph ê³µì‹ë¬¸ì„œ",
                "content_length": len(content),
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {title} ({len(content)}ì)")
            return doc_data
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
            return None
    
    def crawl_guide_pages(self, max_pages: int = 10) -> List[Dict[str, Any]]:
        """ê°€ì´ë“œ í˜ì´ì§€ë“¤ í¬ë¡¤ë§"""
        
        # í¬ë¡¤ë§í•  í•µì‹¬ ê°€ì´ë“œ í˜ì´ì§€ë“¤ (ì‘ê²Œ ì‹œì‘)
        guide_urls = [
            "https://langchain-ai.github.io/langgraph/guides/",
            "https://langchain-ai.github.io/langgraph/how-tos/",
            "https://langchain-ai.github.io/langgraph/concepts/",
            "https://langchain-ai.github.io/langgraph/tutorials/",
            # í•µì‹¬ ê°€ì´ë“œë“¤
            "https://langchain-ai.github.io/langgraph/concepts/low_level/",
            "https://langchain-ai.github.io/langgraph/concepts/persistence/",
            "https://langchain-ai.github.io/langgraph/concepts/streaming/",
            "https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/",
            "https://langchain-ai.github.io/langgraph/how-tos/state-model/",
            "https://langchain-ai.github.io/langgraph/how-tos/persistence/"
        ]
        
        docs = []
        
        for i, url in enumerate(guide_urls[:max_pages]):
            if i > 0:
                time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                
            doc_data = self.get_page_content(url)
            if doc_data and len(doc_data['content']) > 100:  # ìµœì†Œ ì½˜í…ì¸  ê¸¸ì´ í™•ì¸
                docs.append(doc_data)
            
        logger.info(f"ì´ {len(docs)}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ")
        return docs
    
    def save_docs(self, docs: List[Dict[str, Any]]) -> str:
        """í¬ë¡¤ë§ëœ ë¬¸ì„œë“¤ì„ íŒŒì¼ë¡œ ì €ì¥"""
        
        # JSON Lines í˜•ì‹ìœ¼ë¡œ ì €ì¥
        jsonl_file = self.output_dir / "langgraph_docs.jsonl"
        
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            for doc in docs:
                f.write(json.dumps(doc, ensure_ascii=False) + '\n')
        
        # ìš”ì•½ ì •ë³´ ì €ì¥
        summary = {
            "total_docs": len(docs),
            "total_content_length": sum(len(doc['content']) for doc in docs),
            "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "source": "LangGraph ê³µì‹ë¬¸ì„œ"
        }
        
        summary_file = self.output_dir / "crawl_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë¬¸ì„œ ì €ì¥ ì™„ë£Œ: {jsonl_file}")
        logger.info(f"ìš”ì•½ ì •ë³´: {len(docs)}ê°œ ë¬¸ì„œ, ì´ {summary['total_content_length']:,}ì")
        
        return str(jsonl_file)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ LangGraph ê³µì‹ë¬¸ì„œ í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 50)
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = LangGraphDocsCrawler()
    
    # ê°€ì´ë“œ í˜ì´ì§€ë“¤ í¬ë¡¤ë§ (10ê°œ í˜ì´ì§€ë¡œ ì‹œì‘)
    docs = crawler.crawl_guide_pages(max_pages=10)
    
    if not docs:
        print("âŒ í¬ë¡¤ë§ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê²°ê³¼ ì €ì¥
    saved_file = crawler.save_docs(docs)
    
    print("\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"ğŸ“„ ìˆ˜ì§‘ëœ ë¬¸ì„œ: {len(docs)}ê°œ")
    print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {saved_file}")
    
    # ìƒ˜í”Œ ë¬¸ì„œ ì¶œë ¥
    if docs:
        sample = docs[0]
        print(f"\nğŸ“– ìƒ˜í”Œ ë¬¸ì„œ:")
        print(f"ì œëª©: {sample['title']}")
        print(f"URL: {sample['url']}")
        print(f"ë‚´ìš© ê¸¸ì´: {len(sample['content'])}ì")
        print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {sample['content'][:200]}...")


if __name__ == "__main__":
    main() 