#   LangGraph í™œìš© - Multi-Agent ì•„í‚¤í…ì²˜

---

## í™˜ê²½ ì„¤ì • ë° ì¤€ë¹„

`(1) Env í™˜ê²½ë³€ìˆ˜`


```python
from dotenv import load_dotenv
load_dotenv()
```




    True



`(2) ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬`


```python
import os
from glob import glob

from pprint import pprint
import json

import warnings
warnings.filterwarnings("ignore")
```

`(3) Langsmith tracing ì„¤ì •`


```python
# Langsmith tracing ì—¬ë¶€ë¥¼ í™•ì¸ (true: langsmith ì¶”ì²™ í™œì„±í™”, false: langsmith ì¶”ì²™ ë¹„í™œì„±í™”)
import os
print(os.getenv('LANGSMITH_TRACING'))
```

    true


---

## **LangGraph ë©€í‹°ì—ì´ì „íŠ¸**


**1. ì—ì´ì „íŠ¸ë€?**
  - **ì—ì´ì „íŠ¸**ëŠ” LLMì„ ì‚¬ìš©í•˜ì—¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì œì–´ íë¦„ì„ ê²°ì •í•˜ëŠ” ì‹œìŠ¤í…œ

**2. ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì´ í•„ìš”í•œ ì´ìœ **
  - ë‹¨ì¼ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì´ ë³µì¡í•´ì§€ë©´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œê°€ ë°œìƒ:
    - **ë„êµ¬ ê³¼ë¶€í•˜**: ì—ì´ì „íŠ¸ê°€ ë„ˆë¬´ ë§ì€ ë„êµ¬ë¥¼ ê°€ì ¸ ì˜ëª»ëœ ê²°ì •ì„ ë‚´ë¦¼
    - **ì»¨í…ìŠ¤íŠ¸ ë³µì¡ì„±**: ë‹¨ì¼ ì—ì´ì „íŠ¸ê°€ ì¶”ì í•˜ê¸°ì— ë„ˆë¬´ ë³µì¡í•œ ì»¨í…ìŠ¤íŠ¸
    - **ì „ë¬¸í™” í•„ìš”**: í”Œë˜ë„ˆ, ì—°êµ¬ì, ìˆ˜í•™ ì „ë¬¸ê°€ ë“± ì—¬ëŸ¬ ì „ë¬¸ ì˜ì—­ì´ í•„ìš”

**3. ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ì£¼ìš” ì¥ì **
  - **ëª¨ë“ˆì„±**: ê°œë³„ ì—ì´ì „íŠ¸ë¡œ ë¶„ë¦¬í•˜ì—¬ ê°œë°œ, í…ŒìŠ¤íŠ¸, ìœ ì§€ë³´ìˆ˜ê°€ ìš©ì´
  - **ì „ë¬¸í™”**: íŠ¹ì • ë„ë©”ì¸ì— ì´ˆì ì„ ë§ì¶˜ ì „ë¬¸ ì—ì´ì „íŠ¸ ìƒì„±ìœ¼ë¡œ ì „ì²´ ì„±ëŠ¥ í–¥ìƒ
  - **ì œì–´**: ì—ì´ì „íŠ¸ ê°„ í†µì‹ ì„ ëª…ì‹œì ìœ¼ë¡œ ì œì–´ ê°€ëŠ¥

### **1. Supervisor íŒ¨í„´** 

- **íŠ¹ì§•**: ë‹¨ì¼ ìŠˆí¼ë°”ì´ì € ì—ì´ì „íŠ¸ê°€ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ì˜ ì‹¤í–‰ì„ ê²°ì •
- **ì ìš©**: ì¤‘ì•™ ì§‘ì¤‘ì‹ ì œì–´ê°€ í•„ìš”í•œ ê²½ìš°

![Supervisor íŒ¨í„´](https://langchain-ai.github.io/langgraph/agents/assets/supervisor.png)

`(1) langgraph-supervisor íŒ¨í‚¤ì§€ ì‚¬ìš©`

- **ì„¤ì¹˜**

    ```bash
    pip install langgraph-supervisor 
    ```

    ```bash
    uv add langgraph-supervisor
    ```


```python
from langgraph.prebuilt import create_react_agent
from langchain_tavily import TavilySearch
from langchain_core.tools import tool
from IPython.display import Image, display

# 1. ì‘ì—…ì ì—ì´ì „íŠ¸ë“¤ ìƒì„±
# ì—°êµ¬ ì—ì´ì „íŠ¸

tavily_search = TavilySearch(max_results=3)

research_agent = create_react_agent(
    model="openai:gpt-4.1-mini",
    tools=[tavily_search],
    prompt="ë‹¹ì‹ ì€ ì—°êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.",
    name="research_agent"
)

# ìˆ˜í•™ ì—ì´ì „íŠ¸
@tool
def add(a: float, b: float) -> float:
    """ë‘ ìˆ˜ë¥¼ ë”í•©ë‹ˆë‹¤."""
    return a + b

@tool  
def multiply(a: float, b: float) -> float:
    """ë‘ ìˆ˜ë¥¼ ê³±í•©ë‹ˆë‹¤."""
    return a * b

@tool
def divide(a: float, b: float) -> float:
    """ë‘ ìˆ˜ë¥¼ ë‚˜ëˆ•ë‹ˆë‹¤."""
    if b == 0:
        return "0ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    return a / b

math_agent = create_react_agent(
    model="openai:gpt-4.1-mini",
    tools=[add, multiply, divide],
    prompt="ë‹¹ì‹ ì€ ìˆ˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê³„ì‚°ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤.",
    name="math_agent"
)

# 2. ê°ë…ì ì‹œìŠ¤í…œ ìƒì„± (ê°„ë‹¨í•œ ë°©ë²•: langgraph-supervisor íŒ¨í‚¤ì§€ ì‚¬ìš©)
from langgraph_supervisor import create_supervisor
from langchain.chat_models import init_chat_model

supervisor = create_supervisor(
    model=init_chat_model("openai:gpt-4.1"),
    agents=[research_agent, math_agent],
    prompt="""
    ë‹¹ì‹ ì€ ê°ë…ìì…ë‹ˆë‹¤. ë‘ ëª…ì˜ ì—ì´ì „íŠ¸ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤:
    - research_agent: ì •ë³´ ê²€ìƒ‰ ì‘ì—…ì„ ì „ë‹´ 
    - math_agent: ìˆ˜í•™ ê³„ì‚° ì‘ì—…ì„ ì „ë‹´

    ì „ë¬¸ì„±ì„ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ í• ë‹¹í•˜ì„¸ìš”.
    """,
).compile()
    

# ê·¸ë˜í”„ ì‹œê°í™”
display(Image(supervisor.get_graph(xray=False).draw_mermaid_png()))
```


    
![png](/Users/jussuit/Desktop/temp/data/processed/markdown/day6/DAY06_004_LangGraph_Multi-Agent_11_0.png)
    



```python
# 3. ì‹¤í–‰
result = supervisor.invoke({
    "messages": [{
        "role": "user", 
        "content": "í•œêµ­ì˜ ì¸êµ¬ë¥¼ ì°¾ì•„ì„œ, ì¸êµ¬ ìˆ˜ì— 2ë¥¼ ê³±í•´ì£¼ì„¸ìš”."
    }]
})

for m in result["messages"]:
    m.pretty_print()
```

    Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel is_last_step, ignoring it.
    Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel remaining_steps, ignoring it.
    Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel is_last_step, ignoring it.
    Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel remaining_steps, ignoring it.



    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    Cell In[21], line 2
          1 # 3. ì‹¤í–‰
    ----> 2 result = supervisor.invoke({
          3     "messages": [{
          4         "role": "user", 
          5         "content": "í•œêµ­ì˜ ì¸êµ¬ë¥¼ ì°¾ì•„ì„œ, ì¸êµ¬ ìˆ˜ì— 2ë¥¼ ê³±í•´ì£¼ì„¸ìš”."
          6     }]
          7 })
          9 for m in result["messages"]:
         10     m.pretty_print()


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2844, in Pregel.invoke(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)
       2841 chunks: list[dict[str, Any] | Any] = []
       2842 interrupts: list[Interrupt] = []
    -> 2844 for chunk in self.stream(
       2845     input,
       2846     config,
       2847     stream_mode=["updates", "values"]
       2848     if stream_mode == "values"
       2849     else stream_mode,
       2850     print_mode=print_mode,
       2851     output_keys=output_keys,
       2852     interrupt_before=interrupt_before,
       2853     interrupt_after=interrupt_after,
       2854     **kwargs,
       2855 ):
       2856     if stream_mode == "values":
       2857         if len(chunk) == 2:


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2534, in Pregel.stream(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)
       2532 for task in loop.match_cached_writes():
       2533     loop.output_writes(task.id, task.writes, cached=True)
    -> 2534 for _ in runner.tick(
       2535     [t for t in loop.tasks.values() if not t.writes],
       2536     timeout=self.step_timeout,
       2537     get_waiter=get_waiter,
       2538     schedule_task=loop.accept_push,
       2539 ):
       2540     # emit output
       2541     yield from _output(
       2542         stream_mode, print_mode, subgraphs, stream.get, queue.Empty
       2543     )
       2544 loop.after_tick()


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:162, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)
        160 t = tasks[0]
        161 try:
    --> 162     run_with_retry(
        163         t,
        164         retry_policy,
        165         configurable={
        166             CONFIG_KEY_CALL: partial(
        167                 _call,
        168                 weakref.ref(t),
        169                 retry_policy=retry_policy,
        170                 futures=weakref.ref(futures),
        171                 schedule_task=schedule_task,
        172                 submit=self.submit,
        173             ),
        174         },
        175     )
        176     self.commit(t, None)
        177 except Exception as exc:


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:42, in run_with_retry(task, retry_policy, configurable)
         40     task.writes.clear()
         41     # run the task
    ---> 42     return task.proc.invoke(task.input, config)
         43 except ParentCommand as exc:
         44     ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:623, in RunnableSeq.invoke(self, input, config, **kwargs)
        621     # run in context
        622     with set_config_context(config, run) as context:
    --> 623         input = context.run(step.invoke, input, config, **kwargs)
        624 else:
        625     input = step.invoke(input, config)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2844, in Pregel.invoke(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)
       2841 chunks: list[dict[str, Any] | Any] = []
       2842 interrupts: list[Interrupt] = []
    -> 2844 for chunk in self.stream(
       2845     input,
       2846     config,
       2847     stream_mode=["updates", "values"]
       2848     if stream_mode == "values"
       2849     else stream_mode,
       2850     print_mode=print_mode,
       2851     output_keys=output_keys,
       2852     interrupt_before=interrupt_before,
       2853     interrupt_after=interrupt_after,
       2854     **kwargs,
       2855 ):
       2856     if stream_mode == "values":
       2857         if len(chunk) == 2:


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2534, in Pregel.stream(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)
       2532 for task in loop.match_cached_writes():
       2533     loop.output_writes(task.id, task.writes, cached=True)
    -> 2534 for _ in runner.tick(
       2535     [t for t in loop.tasks.values() if not t.writes],
       2536     timeout=self.step_timeout,
       2537     get_waiter=get_waiter,
       2538     schedule_task=loop.accept_push,
       2539 ):
       2540     # emit output
       2541     yield from _output(
       2542         stream_mode, print_mode, subgraphs, stream.get, queue.Empty
       2543     )
       2544 loop.after_tick()


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:162, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)
        160 t = tasks[0]
        161 try:
    --> 162     run_with_retry(
        163         t,
        164         retry_policy,
        165         configurable={
        166             CONFIG_KEY_CALL: partial(
        167                 _call,
        168                 weakref.ref(t),
        169                 retry_policy=retry_policy,
        170                 futures=weakref.ref(futures),
        171                 schedule_task=schedule_task,
        172                 submit=self.submit,
        173             ),
        174         },
        175     )
        176     self.commit(t, None)
        177 except Exception as exc:


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:42, in run_with_retry(task, retry_policy, configurable)
         40     task.writes.clear()
         41     # run the task
    ---> 42     return task.proc.invoke(task.input, config)
         43 except ParentCommand as exc:
         44     ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:623, in RunnableSeq.invoke(self, input, config, **kwargs)
        621     # run in context
        622     with set_config_context(config, run) as context:
    --> 623         input = context.run(step.invoke, input, config, **kwargs)
        624 else:
        625     input = step.invoke(input, config)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:370, in RunnableCallable.invoke(self, input, config, **kwargs)
        368     # run in context
        369     with set_config_context(child_config, run) as context:
    --> 370         ret = context.run(self.func, *args, **kwargs)
        371 except BaseException as e:
        372     run_manager.on_chain_error(e)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py:507, in create_react_agent.<locals>.call_model(state, config)
        505 def call_model(state: StateSchema, config: RunnableConfig) -> StateSchema:
        506     state = _get_model_input_state(state)
    --> 507     response = cast(AIMessage, model_runnable.invoke(state, config))
        508     # add agent name to the AIMessage
        509     response.name = name


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3047, in RunnableSequence.invoke(self, input, config, **kwargs)
       3045                 input_ = context.run(step.invoke, input_, config, **kwargs)
       3046             else:
    -> 3047                 input_ = context.run(step.invoke, input_, config)
       3048 # finish the root run
       3049 except BaseException as e:


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5431, in RunnableBindingBase.invoke(self, input, config, **kwargs)
       5424 @override
       5425 def invoke(
       5426     self,
       (...)   5429     **kwargs: Optional[Any],
       5430 ) -> Output:
    -> 5431     return self.bound.invoke(
       5432         input,
       5433         self._merge_configs(config),
       5434         **{**self.kwargs, **kwargs},
       5435     )


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1326, in ChatGoogleGenerativeAI.invoke(self, input, config, code_execution, stop, **kwargs)
       1321     else:
       1322         raise ValueError(
       1323             "Tools are already defined." "code_execution tool can't be defined"
       1324         )
    -> 1326 return super().invoke(input, config, stop=stop, **kwargs)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:378, in BaseChatModel.invoke(self, input, config, stop, **kwargs)
        366 @override
        367 def invoke(
        368     self,
       (...)    373     **kwargs: Any,
        374 ) -> BaseMessage:
        375     config = ensure_config(config)
        376     return cast(
        377         "ChatGeneration",
    --> 378         self.generate_prompt(
        379             [self._convert_input(input)],
        380             stop=stop,
        381             callbacks=config.get("callbacks"),
        382             tags=config.get("tags"),
        383             metadata=config.get("metadata"),
        384             run_name=config.get("run_name"),
        385             run_id=config.pop("run_id", None),
        386             **kwargs,
        387         ).generations[0][0],
        388     ).message


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:963, in BaseChatModel.generate_prompt(self, prompts, stop, callbacks, **kwargs)
        954 @override
        955 def generate_prompt(
        956     self,
       (...)    960     **kwargs: Any,
        961 ) -> LLMResult:
        962     prompt_messages = [p.to_messages() for p in prompts]
    --> 963     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:782, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)
        779 for i, m in enumerate(input_messages):
        780     try:
        781         results.append(
    --> 782             self._generate_with_cache(
        783                 m,
        784                 stop=stop,
        785                 run_manager=run_managers[i] if run_managers else None,
        786                 **kwargs,
        787             )
        788         )
        789     except BaseException as e:
        790         if run_managers:


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1028, in BaseChatModel._generate_with_cache(self, messages, stop, run_manager, **kwargs)
       1026     result = generate_from_stream(iter(chunks))
       1027 elif inspect.signature(self._generate).parameters.get("run_manager"):
    -> 1028     result = self._generate(
       1029         messages, stop=stop, run_manager=run_manager, **kwargs
       1030     )
       1031 else:
       1032     result = self._generate(messages, stop=stop, **kwargs)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1433, in ChatGoogleGenerativeAI._generate(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)
       1406 def _generate(
       1407     self,
       1408     messages: List[BaseMessage],
       (...)   1419     **kwargs: Any,
       1420 ) -> ChatResult:
       1421     request = self._prepare_request(
       1422         messages,
       1423         stop=stop,
       (...)   1431         **kwargs,
       1432     )
    -> 1433     response: GenerateContentResponse = _chat_with_retry(
       1434         request=request,
       1435         **kwargs,
       1436         generation_method=self.client.generate_content,
       1437         metadata=self.default_metadata,
       1438     )
       1439     return _response_to_result(response)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:231, in _chat_with_retry(generation_method, **kwargs)
        222         raise e
        224 params = (
        225     {k: v for k, v in kwargs.items() if k in _allowed_params_prediction_service}
        226     if (request := kwargs.get("request"))
       (...)    229     else kwargs
        230 )
    --> 231 return _chat_with_retry(**params)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/tenacity/__init__.py:338, in BaseRetrying.wraps.<locals>.wrapped_f(*args, **kw)
        336 copy = self.copy()
        337 wrapped_f.statistics = copy.statistics  # type: ignore[attr-defined]
    --> 338 return copy(f, *args, **kw)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/tenacity/__init__.py:477, in Retrying.__call__(self, fn, *args, **kwargs)
        475 retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)
        476 while True:
    --> 477     do = self.iter(retry_state=retry_state)
        478     if isinstance(do, DoAttempt):
        479         try:


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/tenacity/__init__.py:378, in BaseRetrying.iter(self, retry_state)
        376 result = None
        377 for action in self.iter_state.actions:
    --> 378     result = action(retry_state)
        379 return result


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/tenacity/__init__.py:400, in BaseRetrying._post_retry_check_actions.<locals>.<lambda>(rs)
        398 def _post_retry_check_actions(self, retry_state: "RetryCallState") -> None:
        399     if not (self.iter_state.is_explicit_retry or self.iter_state.retry_run_result):
    --> 400         self._add_action_func(lambda rs: rs.outcome.result())
        401         return
        403     if self.after is not None:


    File ~/miniconda3/lib/python3.12/concurrent/futures/_base.py:449, in Future.result(self, timeout)
        447     raise CancelledError()
        448 elif self._state == FINISHED:
    --> 449     return self.__get_result()
        451 self._condition.wait(timeout)
        453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:


    File ~/miniconda3/lib/python3.12/concurrent/futures/_base.py:401, in Future.__get_result(self)
        399 if self._exception:
        400     try:
    --> 401         raise self._exception
        402     finally:
        403         # Break a reference cycle with the exception in self._exception
        404         self = None


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/tenacity/__init__.py:480, in Retrying.__call__(self, fn, *args, **kwargs)
        478 if isinstance(do, DoAttempt):
        479     try:
    --> 480         result = fn(*args, **kwargs)
        481     except BaseException:  # noqa: B902
        482         retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:206, in _chat_with_retry.<locals>._chat_with_retry(**kwargs)
        203 @retry_decorator
        204 def _chat_with_retry(**kwargs: Any) -> Any:
        205     try:
    --> 206         return generation_method(**kwargs)
        207     # Do not retry for these errors.
        208     except google.api_core.exceptions.FailedPrecondition as exc:


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:868, in GenerativeServiceClient.generate_content(self, request, model, contents, retry, timeout, metadata)
        865 self._validate_universe_domain()
        867 # Send the request.
    --> 868 response = rpc(
        869     request,
        870     retry=retry,
        871     timeout=timeout,
        872     metadata=metadata,
        873 )
        875 # Done; return the response.
        876 return response


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131, in _GapicCallable.__call__(self, timeout, retry, compression, *args, **kwargs)
        128 if self._compression is not None:
        129     kwargs["compression"] = compression
    --> 131 return wrapped_func(*args, **kwargs)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:294, in Retry.__call__.<locals>.retry_wrapped_func(*args, **kwargs)
        290 target = functools.partial(func, *args, **kwargs)
        291 sleep_generator = exponential_sleep_generator(
        292     self._initial, self._maximum, multiplier=self._multiplier
        293 )
    --> 294 return retry_target(
        295     target,
        296     self._predicate,
        297     sleep_generator,
        298     timeout=self._timeout,
        299     on_error=on_error,
        300 )


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:147, in retry_target(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)
        145 while True:
        146     try:
    --> 147         result = target()
        148         if inspect.isawaitable(result):
        149             warnings.warn(_ASYNC_RETRY_WARNING)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/google/api_core/timeout.py:130, in TimeToDeadlineTimeout.__call__.<locals>.func_with_timeout(*args, **kwargs)
        126         remaining_timeout = self._timeout
        128     kwargs["timeout"] = remaining_timeout
    --> 130 return func(*args, **kwargs)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:76, in _wrap_unary_errors.<locals>.error_remapped_callable(*args, **kwargs)
         73 @functools.wraps(callable_)
         74 def error_remapped_callable(*args, **kwargs):
         75     try:
    ---> 76         return callable_(*args, **kwargs)
         77     except grpc.RpcError as exc:
         78         raise exceptions.from_grpc_error(exc) from exc


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/grpc/_interceptor.py:277, in _UnaryUnaryMultiCallable.__call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)
        268 def __call__(
        269     self,
        270     request: Any,
       (...)    275     compression: Optional[grpc.Compression] = None,
        276 ) -> Any:
    --> 277     response, ignored_call = self._with_call(
        278         request,
        279         timeout=timeout,
        280         metadata=metadata,
        281         credentials=credentials,
        282         wait_for_ready=wait_for_ready,
        283         compression=compression,
        284     )
        285     return response


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/grpc/_interceptor.py:329, in _UnaryUnaryMultiCallable._with_call(self, request, timeout, metadata, credentials, wait_for_ready, compression)
        326     except Exception as exception:  # pylint:disable=broad-except
        327         return _FailureOutcome(exception, sys.exc_info()[2])
    --> 329 call = self._interceptor.intercept_unary_unary(
        330     continuation, client_call_details, request
        331 )
        332 return call.result(), call


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:78, in _LoggingClientInterceptor.intercept_unary_unary(self, continuation, client_call_details, request)
         64     grpc_request = {
         65         "payload": request_payload,
         66         "requestMethod": "grpc",
         67         "metadata": dict(request_metadata),
         68     }
         69     _LOGGER.debug(
         70         f"Sending request for {client_call_details.method}",
         71         extra={
       (...)     76         },
         77     )
    ---> 78 response = continuation(client_call_details, request)
         79 if logging_enabled:  # pragma: NO COVER
         80     response_metadata = response.trailing_metadata()


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/grpc/_interceptor.py:315, in _UnaryUnaryMultiCallable._with_call.<locals>.continuation(new_details, request)
        306 (
        307     new_method,
        308     new_timeout,
       (...)    312     new_compression,
        313 ) = _unwrap_client_call_details(new_details, client_call_details)
        314 try:
    --> 315     response, call = self._thunk(new_method).with_call(
        316         request,
        317         timeout=new_timeout,
        318         metadata=new_metadata,
        319         credentials=new_credentials,
        320         wait_for_ready=new_wait_for_ready,
        321         compression=new_compression,
        322     )
        323     return _UnaryOutcome(response, call)
        324 except grpc.RpcError as rpc_error:


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/grpc/_channel.py:1195, in _UnaryUnaryMultiCallable.with_call(self, request, timeout, metadata, credentials, wait_for_ready, compression)
       1183 def with_call(
       1184     self,
       1185     request: Any,
       (...)   1190     compression: Optional[grpc.Compression] = None,
       1191 ) -> Tuple[Any, grpc.Call]:
       1192     (
       1193         state,
       1194         call,
    -> 1195     ) = self._blocking(
       1196         request, timeout, metadata, credentials, wait_for_ready, compression
       1197     )
       1198     return _end_unary_response_blocking(state, call, True, None)


    File ~/Edu/modulab/llm-skt/.venv/lib/python3.12/site-packages/grpc/_channel.py:1162, in _UnaryUnaryMultiCallable._blocking(self, request, timeout, metadata, credentials, wait_for_ready, compression)
       1145 state.target = _common.decode(self._target)
       1146 call = self._channel.segregated_call(
       1147     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,
       1148     self._method,
       (...)   1160     self._registered_call_handle,
       1161 )
    -> 1162 event = call.next_event()
       1163 _handle_event(event, state, self._response_deserializer)
       1164 return state, call


    File src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388, in grpc._cython.cygrpc.SegregatedCall.next_event()


    File src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211, in grpc._cython.cygrpc._next_call_event()


    File src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205, in grpc._cython.cygrpc._next_call_event()


    File src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97, in grpc._cython.cygrpc._latent_event()


    File src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80, in grpc._cython.cygrpc._internal_latent_event()


    File src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61, in grpc._cython.cygrpc._next()


    KeyboardInterrupt: 


`(2) ì»¤ìŠ¤í…€ í•¸ë“œì˜¤í”„ ë„êµ¬ ì‚¬ìš©`

- **í•¸ë“œì˜¤í”„ ë„êµ¬**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ ê°„ í†µì‹ ì„ ëª…ì‹œì ìœ¼ë¡œ ì œì–´ (í˜„ì¬ ì—ì´ì „íŠ¸ì—ì„œ ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ ì´ë™í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë„êµ¬)
- ì´ë•Œ, graph ì¸ìë¥¼ **Command.PARENT**ë¡œ ì„¤ì •í•˜ì—¬ ë¶€ëª¨ ê·¸ë˜í”„ë¡œ ëŒì•„ê°€ê¸°


```python
from typing import Annotated
from langchain_core.tools import tool, InjectedToolCallId
from langchain_core.messages import ToolMessage
from langgraph.prebuilt import InjectedState
from langgraph.graph import MessagesState
from langgraph.types import Command

# í•¸ë“œì˜¤í”„ ë„êµ¬ ìƒì„± í•¨ìˆ˜
def create_handoff_tool(*, agent_name: str, description: str | None = None):
    """ì»¤ìŠ¤í…€ í•¸ë“œì˜¤í”„ ë„êµ¬ ìƒì„±"""
    name = f"transfer_to_{agent_name}"   # ë‹¤ìŒì— ì´ë™í•  ì—ì´ì „íŠ¸ ì´ë¦„ (ì‘ì—… í• ë‹¹)
    description = description or f"Transfer to {agent_name}"   # ì—ì´ì „íŠ¸ ì´ë™ì— ëŒ€í•œ ì„¤ëª… (ì‘ì—… í• ë‹¹)

    @tool(name, description=description)
    def handoff_tool(
        state: Annotated[MessagesState, InjectedState], 
        tool_call_id: Annotated[str, InjectedToolCallId],
    ) -> Command:
        
        # ë„êµ¬ í˜¸ì¶œ ë©”ì‹œì§€ ìƒì„±
        tool_message = ToolMessage(
            content=f"Successfully transferred to {agent_name}",
            name=name,
            tool_call_id=tool_call_id,
        )

        # ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ ì´ë™í•˜ê³  ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
        return Command(  
            goto=agent_name,  # ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ ì´ë™ (ì‘ì—… í• ë‹¹)
            update={"messages": state["messages"] + [tool_message]},   # ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ (ì‘ì—… í• ë‹¹)
            graph=Command.PARENT,  # ë¶€ëª¨ ê·¸ë˜í”„ë¡œ ëŒì•„ê°€ê¸° (ì‘ì—… ì™„ë£Œ í›„ ìŠˆí¼ë°”ì´ì €ì—ê²Œ ë³´ê³ )
        )
    return handoff_tool
```


```python
from langgraph.graph import MessagesState, StateGraph, START, END
from langchain.chat_models import init_chat_model
# from langgraph_supervisor import create_handoff_tool


# í•¸ë“œì˜¤í”„ ë„êµ¬ (supervisor â†’ research_agent)
transfer_to_research_agent = create_handoff_tool(
    agent_name="research_agent",
    description="ì •ë³´ ê²€ìƒ‰, ì¡°ì‚¬, ì°¾ê¸° ì‘ì—…ì„ ì—°êµ¬ ì „ë¬¸ê°€ì—ê²Œ í• ë‹¹"
)

# í•¸ë“œì˜¤í”„ ë„êµ¬ (supervisor â†’ math_agent)
transfer_to_math_agent = create_handoff_tool(
    agent_name="math_agent", 
    description="ìˆ˜í•™ ê³„ì‚°, ê³±ì…ˆ, ë§ì…ˆ ì‘ì—…ì„ ìˆ˜í•™ ì „ë¬¸ê°€ì—ê²Œ í• ë‹¹"
)

# í•¸ë“œì˜¤í”„ ë„êµ¬ (research_agent, math_agent â†’ supervisor)
transfer_to_supervisor = create_handoff_tool(
    agent_name="supervisor",
    description="ì‘ì—… ì™„ë£Œ í›„ ìŠˆí¼ë°”ì´ì €ì—ê²Œ ë³´ê³ "
)

# ìŠˆí¼ë°”ì´ì € ì—ì´ì „íŠ¸
supervisor = create_react_agent(
    model=init_chat_model("openai:gpt-4.1"),
    tools=[transfer_to_research_agent, transfer_to_math_agent],
    prompt="""ë‹¹ì‹ ì€ íŒ€ ìŠˆí¼ë°”ì´ì €ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì „ë¬¸ê°€ì—ê²Œ ì‘ì—…ì„ í• ë‹¹í•˜ì„¸ìš”.

ğŸ” **ì—°êµ¬ ì‘ì—…** (research_agent):
- ì •ë³´ ê²€ìƒ‰, ì¡°ì‚¬, ì°¾ê¸°
- ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì‘ì—…
- ë°ì´í„° ìˆ˜ì§‘

ğŸ§® **ìˆ˜í•™ ì‘ì—…** (math_agent):  
- ê³„ì‚°, ê³±ì…ˆ, ë§ì…ˆ, ë‚˜ëˆ—ì…ˆ
- ìˆ˜ì¹˜ ì²˜ë¦¬

ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ê³  transfer_to_research_agent ë˜ëŠ” transfer_to_math_agent ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ í• ë‹¹í•˜ì„¸ìš”.
ë³µí•© ì‘ì—…ì˜ ê²½ìš° ë¨¼ì € ì •ë³´ ìˆ˜ì§‘ë¶€í„° ì‹œì‘í•˜ì„¸ìš”.""",
    name="supervisor"
)

# ì—°êµ¬ ì—ì´ì „íŠ¸ (ìŠˆí¼ë°”ì´ì €ì—ê²Œ ë³µê·€)
research_agent = create_react_agent(
    model=init_chat_model("openai:gpt-4.1-mini"),
    tools=[tavily_search, transfer_to_supervisor],
    prompt="""ë‹¹ì‹ ì€ ì—°êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ í†µí•´ ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì•„ ì œê³µí•˜ì„¸ìš”.

ì‘ì—…ì„ ì™„ë£Œí•œ í›„ì—ëŠ” ë°˜ë“œì‹œ transfer_to_supervisor ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ìŠˆí¼ë°”ì´ì €ì—ê²Œ ê²°ê³¼ë¥¼ ë³´ê³ í•˜ì„¸ìš”.""",
    name="research_agent"
)

# ìˆ˜í•™ ì—ì´ì „íŠ¸ (ìŠˆí¼ë°”ì´ì €ì—ê²Œ ë³µê·€)
math_agent = create_react_agent(
    model=init_chat_model("openai:gpt-4.1-mini"),
    tools=[add, multiply, divide, transfer_to_supervisor],
    prompt="""ë‹¹ì‹ ì€ ìˆ˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•œ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

ì‘ì—…ì„ ì™„ë£Œí•œ í›„ì—ëŠ” ë°˜ë“œì‹œ transfer_to_supervisor ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ìŠˆí¼ë°”ì´ì €ì—ê²Œ ê²°ê³¼ë¥¼ ë³´ê³ í•˜ì„¸ìš”.""",
    name="math_agent"
)

# ê·¸ë˜í”„ êµ¬ì„±
supervisor_graph = (
    StateGraph(MessagesState)
    .add_node("supervisor", supervisor)
    .add_node("research_agent", research_agent)
    .add_node("math_agent", math_agent)
    .add_edge(START, "supervisor")  # í•­ìƒ ìŠˆí¼ë°”ì´ì €ë¶€í„° ì‹œì‘    
    .compile()
)

```


```python
# ì‹¤í–‰
result = supervisor_graph.invoke({
    "messages": [{
        "role": "user", 
        "content": "í•œêµ­ì˜ ì¸êµ¬ë¥¼ ì°¾ì•„ì„œ, ì¸êµ¬ ìˆ˜ì— 2ë¥¼ ê³±í•´ì£¼ì„¸ìš”."
    }]
})

for m in result["messages"]:
    m.pretty_print()
```

    ================================[1m Human Message [0m=================================
    
    í•œêµ­ì˜ ì¸êµ¬ë¥¼ ì°¾ì•„ì„œ, ì¸êµ¬ ìˆ˜ì— 2ë¥¼ ê³±í•´ì£¼ì„¸ìš”.
    ==================================[1m Ai Message [0m==================================
    Name: supervisor
    Tool Calls:
      transfer_to_research_agent (call_cqoWtyp2duH5vzpMXMyc1ciu)
     Call ID: call_cqoWtyp2duH5vzpMXMyc1ciu
      Args:
    =================================[1m Tool Message [0m=================================
    Name: transfer_to_research_agent
    
    Successfully transferred to research_agent
    ==================================[1m Ai Message [0m==================================
    Name: research_agent
    Tool Calls:
      tavily_search (call_b6iW2ZYfZJoRDICwdu4x3d5x)
     Call ID: call_b6iW2ZYfZJoRDICwdu4x3d5x
      Args:
        query: í•œêµ­ ì¸êµ¬
    =================================[1m Tool Message [0m=================================
    Name: tavily_search
    
    {"query": "í•œêµ­ ì¸êµ¬", "follow_up_questions": null, "answer": null, "images": [], "results": [{"url": "https://namu.wiki/w/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD/%EC%9D%B8%EA%B5%AC", "title": "ëŒ€í•œë¯¼êµ­/ì¸êµ¬ - ë‚˜ë¬´ìœ„í‚¤", "content": "ëŒ€í•œë¯¼êµ­ì˜ ì¸êµ¬ë¥¼ ì •ë¦¬í•œ ë¬¸ì„œë‹¤. 2025ë…„ 6ì›” ê¸°ì¤€ìœ¼ë¡œ ëŒ€í•œë¯¼êµ­ì˜ ì´ ì¸êµ¬ìˆ˜ëŠ” 51,164,582ëª…ì´ë‹¤. ì´ ì¤‘ ë‚¨ì ì¸êµ¬ìˆ˜ëŠ” 25,467,115ëª…ì´ê³ ,", "score": 0.8745175, "raw_content": null}, {"url": "https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%9D%B8%EA%B5%AC", "title": "ëŒ€í•œë¯¼êµ­ì˜ ì¸êµ¬ - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „", "content": "ì¤‘ìœ„ ì¶”ê³„ì— ë”°ë¥´ë©´ í•œêµ­ì€ 2050ë…„ 65ì„¸ ì´ìƒ ë…¸ì¸ ë¹„ìœ¨ì´ 38.1%ì— ë‹¬í•˜ë©°, ê°™ì€ ê¸°ê°„ ì¼ë³¸ 37.7%ì„ ì œì¹˜ê³  ì„¸ê³„ 1ìœ„ ë…¸ì¸ ë¹„ìœ¨ êµ­ê°€ê°€ ë˜ë©°, 2060ë…„ì— ì´ë¥´ëŸ¬ì„œëŠ” 65ì„¸ ì´ìƒ ë…¸ì¸ ì¸êµ¬ê°€ 25~64ì„¸ ì¼í•˜ëŠ” ì¸êµ¬ë³´ë‹¤ ë§ì•„ì ¸ ë…¸ì¸ ë¶€ì–‘ì´ ì‚¬ì‹¤ìƒ ì–´ë ¤ìš¸ ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤. ì €ìœ„ ì¶”ê³„ì— ë”°ë¥´ë©´, í•œêµ­ì€ 65ì„¸ ì´ìƒ ë…¸ì¸ ë¹„ìœ¨ì´ 2045ë…„ ì¼ë³¸ì„ ì¶”ì›”í•´ ì „ ì„¸ê³„ì—ì„œ ê°€ì¥ ë†’ì€ êµ­ê°€ê°€ ë  ê²ƒì´ë©°, 2100ë…„ ì¸êµ¬ê°€ 1928ë§Œ ëª…ìœ¼ë¡œ ê°ì†Œí•  ì „ë§ì´ë‹¤. | ë…ë¦½êµ­ |  ë„¤íŒ”  ëŒ€í•œë¯¼êµ­  ë™í‹°ëª¨ë¥´  ë¼ì˜¤ìŠ¤  ëŸ¬ì‹œì•„  ë ˆë°”ë…¼  ë§ë ˆì´ì‹œì•„  ëª°ë””ë¸Œ  ëª½ê³¨  ë¯¸ì–€ë§ˆ  ë°”ë ˆì¸  ë°©ê¸€ë¼ë°ì‹œ  ë² íŠ¸ë‚¨  ë¶€íƒ„  ë¸Œë£¨ë‚˜ì´  ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„  ìŠ¤ë¦¬ë‘ì¹´  ì‹œë¦¬ì•„  ì‹±ê°€í¬ë¥´  ì•„ëì—ë¯¸ë¦¬íŠ¸  ì•„ë¥´ë©”ë‹ˆì•„  ì•„ì œë¥´ë°”ì´ì”  ì•„í”„ê°€ë‹ˆìŠ¤íƒ„  ì˜ˆë©˜  ì˜¤ë§Œ  ìš”ë¥´ë‹¨  ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„  ì´ë¼í¬  ì´ë€  ì´ìŠ¤ë¼ì—˜  ì´ì§‘íŠ¸  ì¸ë„  ì¸ë„ë„¤ì‹œì•„  ì¼ë³¸  ì¡°ì„ ë¯¼ì£¼ì£¼ì˜ì¸ë¯¼ê³µí™”êµ­  ì¡°ì§€ì•„  ì¤‘í™”ë¯¼êµ­  ì¤‘í™”ì¸ë¯¼ê³µí™”êµ­  ì¹´ìíìŠ¤íƒ„  ì¹´íƒ€ë¥´  ìº„ë³´ë””ì•„  ì¿ ì›¨ì´íŠ¸  í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„  í‚¤í”„ë¡œìŠ¤  íƒ€ì§€í‚¤ìŠ¤íƒ„  íƒœêµ­  íˆ¬ë¥´í¬ë©”ë‹ˆìŠ¤íƒ„  íŠ€ë¥´í‚¤ì˜ˆ  íŒŒí‚¤ìŠ¤íƒ„  íŒ”ë ˆìŠ¤íƒ€ì¸  í•„ë¦¬í•€ |  |", "score": 0.7769112, "raw_content": null}, {"url": "https://kosis.kr/visual/populationKorea/PopulationDashBoardMain.do", "title": "ì¸êµ¬ìƒí™©íŒ | ì¸êµ¬ë¡œ ë³´ëŠ” ëŒ€í•œë¯¼êµ­ - KOSIS êµ­ê°€í†µê³„í¬í„¸", "content": "ì§€ì—­ë³„ ì¸êµ¬ í†µê³„ì— ëŒ€í•œ í˜„í™© ë° ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê´€ê³„ë„ë§µì„ í†µí•´ ì¸êµ¬ì™€ ì‚¬íšŒÂ·ê²½ì œê°„ ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¸êµ¬ í†µê³„ë¥¼ ë” ë‹¤ì–‘í•œ í˜•íƒœë¡œ ê²½í—˜í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. #### 40ì„¸ 1972ë…„ìƒ ë˜í•œ ì¸êµ¬ êµ¬ì¡°ê°€ ì¶œìƒ, ì‚¬ë§, ì´ë™ ë“± ìš”ì¸ì˜ ì‹œë‚˜ë¦¬ì˜¤ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ìƒì„¸í™”ë©´ì—ì„œ ë” ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. #### 40ì„¸ 1972ë…„ìƒ ë˜í•œ ì¸êµ¬ êµ¬ì¡°ê°€ ì¶œìƒ, ì‚¬ë§, ì´ë™ ë“± ìš”ì¸ì˜ ì‹œë‚˜ë¦¬ì˜¤ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ìƒì„¸í™”ë©´ì—ì„œ ë” ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. #### 40ì„¸ 1972ë…„ìƒ ë˜í•œ ì¸êµ¬ êµ¬ì¡°ê°€ ì¶œìƒ, ì‚¬ë§, ì´ë™ ë“± ìš”ì¸ì˜ ì‹œë‚˜ë¦¬ì˜¤ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ìƒì„¸í™”ë©´ì—ì„œ ë” ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ### ì´ì¸êµ¬ (ì¶”ê³„ ê¸°ì¤€) ### í•™ë ¹ì¸êµ¬ (6-21ì„¸) ### 1ì¸ ê°€êµ¬ ### 1ì¸ ê°€êµ¬ ë¹„ì¤‘ ### ì—°ë ¹ëŒ€ë³„ 1ì¸ ê°€êµ¬ (2025) ### ì¥ë˜ ì¶œìƒì•„ìˆ˜ ### ì¥ë˜ ì‚¬ë§ììˆ˜ ### êµ­ì œìˆœì´ë™ ### ì™¸êµ­ì¸ êµ­ì œìˆœì´ë™ ### ì™¸êµ­ì¸ ì·¨ì—…ì", "score": 0.3507332, "raw_content": null}], "response_time": 1.18}
    ==================================[1m Ai Message [0m==================================
    Name: research_agent
    
    í•œêµ­ì˜ ì¸êµ¬ëŠ” 2025ë…„ 6ì›” ê¸°ì¤€ìœ¼ë¡œ ì•½ 51,164,582ëª…ì…ë‹ˆë‹¤. ì´ ì¸êµ¬ ìˆ˜ì— 2ë¥¼ ê³±í•˜ë©´ ì•½ 102,329,164ëª…ì´ ë©ë‹ˆë‹¤.
    Tool Calls:
      transfer_to_supervisor (call_a7MBVcrfzbZI9rYtWmXlesoQ)
     Call ID: call_a7MBVcrfzbZI9rYtWmXlesoQ
      Args:
    =================================[1m Tool Message [0m=================================
    Name: transfer_to_supervisor
    
    Successfully transferred to supervisor
    ==================================[1m Ai Message [0m==================================
    Name: supervisor
    Tool Calls:
      transfer_to_math_agent (call_eBWCRe4Htt9SM7gcJ8543srA)
     Call ID: call_eBWCRe4Htt9SM7gcJ8543srA
      Args:
    =================================[1m Tool Message [0m=================================
    Name: transfer_to_math_agent
    
    Successfully transferred to math_agent
    ==================================[1m Ai Message [0m==================================
    Name: math_agent
    Tool Calls:
      multiply (call_gazZufAqfI99d0RfBORF209M)
     Call ID: call_gazZufAqfI99d0RfBORF209M
      Args:
        a: 51164582
        b: 2
    =================================[1m Tool Message [0m=================================
    Name: multiply
    
    102329164.0
    ==================================[1m Ai Message [0m==================================
    Name: math_agent
    Tool Calls:
      transfer_to_supervisor (call_rOu5Cdwi3ho8MeAI0rPPVeuB)
     Call ID: call_rOu5Cdwi3ho8MeAI0rPPVeuB
      Args:
    =================================[1m Tool Message [0m=================================
    Name: transfer_to_supervisor
    
    Successfully transferred to supervisor
    ==================================[1m Ai Message [0m==================================
    Name: supervisor
    
    í•œêµ­ì˜ ì¸êµ¬(2025ë…„ 6ì›” ê¸°ì¤€)ëŠ” ì•½ 51,164,582ëª…ì…ë‹ˆë‹¤.  
    ì´ ì¸êµ¬ ìˆ˜ì— 2ë¥¼ ê³±í•˜ë©´ 102,329,164ëª…ì´ ë©ë‹ˆë‹¤.


### **2. Swarm íŒ¨í„´** 

- **íŠ¹ì§•**: ë¶„ì‚°í˜•, ì—ì´ì „íŠ¸ ê°„ ììœ¨ì  í˜‘ë ¥
- **ì„¤ì¹˜**
    - langgraph-swarm íŒ¨í‚¤ì§€ ì‚¬ìš©

    ```bash
    pip install langgraph-swarm 
    ```

    ```bash
    uv add langgraph-swarm
    ```

![Swarm íŒ¨í„´](https://langchain-ai.github.io/langgraph/agents/assets/swarm.png)


```python
from langgraph_swarm import create_swarm, create_handoff_tool

# í•¸ë“œì˜¤í”„ ë„êµ¬ ìƒì„±
transfer_to_math_agent = create_handoff_tool(
    agent_name="math_agent",
    description="ìˆ˜í•™ ê³„ì‚°ì´ í•„ìš”í•  ë•Œ ìˆ˜í•™ ì „ë¬¸ê°€ì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤."
)

transfer_to_research_agent = create_handoff_tool(
    agent_name="research_agent", 
    description="ì •ë³´ ê²€ìƒ‰ì´ë‚˜ ì¡°ì‚¬ê°€ í•„ìš”í•  ë•Œ ì—°êµ¬ ì „ë¬¸ê°€ì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤."
)

# ì—°êµ¬ ì—ì´ì „íŠ¸ (í•¸ë“œì˜¤í”„ ë„êµ¬ í¬í•¨)
research_agent = create_react_agent(
    model=init_chat_model("openai:gpt-4.1-mini"),
    tools=[tavily_search, transfer_to_math_agent],
    prompt="""ë‹¹ì‹ ì€ ì—°êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ í†µí•´ ì •ë³´ë¥¼ ì°¾ì•„ ì œê³µí•©ë‹ˆë‹¤.
    
ë§Œì•½ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìˆ«ì ë°ì´í„°ë¥¼ ì°¾ì•˜ê³  ì‚¬ìš©ìê°€ ê³„ì‚°ì„ ìš”ì²­í•œë‹¤ë©´, 
transfer_to_math_agent ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ìˆ˜í•™ ì „ë¬¸ê°€ì—ê²Œ ì‘ì—…ì„ ì „ë‹¬í•˜ì„¸ìš”.""",
    name="research_agent"
)

# ìˆ˜í•™ ì—ì´ì „íŠ¸ (í•¸ë“œì˜¤í”„ ë„êµ¬ í¬í•¨)
math_agent = create_react_agent(
    model=init_chat_model("openai:gpt-4.1-mini"),
    tools=[add, multiply, divide, transfer_to_research_agent],
    prompt="""ë‹¹ì‹ ì€ ìˆ˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•œ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
ë§Œì•½ ê³„ì‚°ì— í•„ìš”í•œ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ë©´,
transfer_to_research_agent ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì—°êµ¬ ì „ë¬¸ê°€ì—ê²Œ ì‘ì—…ì„ ì „ë‹¬í•˜ì„¸ìš”.""",
    name="math_agent"
)

# ìŠ¤ì›œ ìƒì„±
swarm = create_swarm(
    agents=[research_agent, math_agent],
    default_active_agent="research_agent"  # ê¸°ë³¸ì ìœ¼ë¡œ ì—°êµ¬ ì—ì´ì „íŠ¸ê°€ ë¨¼ì € ì‹œì‘
).compile()

# ì‹œê°í™”
display(Image(swarm.get_graph(xray=False).draw_mermaid_png()))
```


    
![png](/Users/jussuit/Desktop/temp/data/processed/markdown/day6/DAY06_004_LangGraph_Multi-Agent_18_0.png)
    



```python
# ì‹¤í–‰
result = swarm.invoke({
    "messages": [{
        "role": "user", 
        "content": "í•œêµ­ì˜ ì¸êµ¬ë¥¼ ì°¾ì•„ì„œ, ì¸êµ¬ ìˆ˜ì— 2ë¥¼ ê³±í•´ì£¼ì„¸ìš”."
    }]
})

for m in result["messages"]:
    m.pretty_print()
```

    ================================[1m Human Message [0m=================================
    
    í•œêµ­ì˜ ì¸êµ¬ë¥¼ ì°¾ì•„ì„œ, ì¸êµ¬ ìˆ˜ì— 2ë¥¼ ê³±í•´ì£¼ì„¸ìš”.
    ==================================[1m Ai Message [0m==================================
    Name: research_agent
    Tool Calls:
      tavily_search (call_PQlzqh1IurwdqrwBOrzl4Gkb)
     Call ID: call_PQlzqh1IurwdqrwBOrzl4Gkb
      Args:
        query: í•œêµ­ ì¸êµ¬ ìˆ˜
    =================================[1m Tool Message [0m=================================
    Name: tavily_search
    
    {"query": "í•œêµ­ ì¸êµ¬ ìˆ˜", "follow_up_questions": null, "answer": null, "images": [], "results": [{"url": "https://namu.wiki/w/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD/%EC%9D%B8%EA%B5%AC", "title": "ëŒ€í•œë¯¼êµ­/ì¸êµ¬ - ë‚˜ë¬´ìœ„í‚¤", "content": "ëŒ€í•œë¯¼êµ­ì˜ ì¸êµ¬ë¥¼ ì •ë¦¬í•œ ë¬¸ì„œë‹¤. 2025ë…„ 6ì›” ê¸°ì¤€ìœ¼ë¡œ ëŒ€í•œë¯¼êµ­ì˜ ì´ ì¸êµ¬ìˆ˜ëŠ” 51,164,582ëª…ì´ë‹¤. ì´ ì¤‘ ë‚¨ì ì¸êµ¬ìˆ˜ëŠ” 25,467,115ëª…ì´ê³ ,", "score": 0.89166987, "raw_content": null}, {"url": "https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%9D%B8%EA%B5%AC", "title": "ëŒ€í•œë¯¼êµ­ì˜ ì¸êµ¬ - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „", "content": "ì¤‘ìœ„ ì¶”ê³„ì— ë”°ë¥´ë©´ í•œêµ­ì€ 2050ë…„ 65ì„¸ ì´ìƒ ë…¸ì¸ ë¹„ìœ¨ì´ 38.1%ì— ë‹¬í•˜ë©°, ê°™ì€ ê¸°ê°„ ì¼ë³¸ 37.7%ì„ ì œì¹˜ê³  ì„¸ê³„ 1ìœ„ ë…¸ì¸ ë¹„ìœ¨ êµ­ê°€ê°€ ë˜ë©°, 2060ë…„ì— ì´ë¥´ëŸ¬ì„œëŠ” 65ì„¸ ì´ìƒ ë…¸ì¸ ì¸êµ¬ê°€ 25~64ì„¸ ì¼í•˜ëŠ” ì¸êµ¬ë³´ë‹¤ ë§ì•„ì ¸ ë…¸ì¸ ë¶€ì–‘ì´ ì‚¬ì‹¤ìƒ ì–´ë ¤ìš¸ ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤. ì €ìœ„ ì¶”ê³„ì— ë”°ë¥´ë©´, í•œêµ­ì€ 65ì„¸ ì´ìƒ ë…¸ì¸ ë¹„ìœ¨ì´ 2045ë…„ ì¼ë³¸ì„ ì¶”ì›”í•´ ì „ ì„¸ê³„ì—ì„œ ê°€ì¥ ë†’ì€ êµ­ê°€ê°€ ë  ê²ƒì´ë©°, 2100ë…„ ì¸êµ¬ê°€ 1928ë§Œ ëª…ìœ¼ë¡œ ê°ì†Œí•  ì „ë§ì´ë‹¤. | ë…ë¦½êµ­ |  ë„¤íŒ”  ëŒ€í•œë¯¼êµ­  ë™í‹°ëª¨ë¥´  ë¼ì˜¤ìŠ¤  ëŸ¬ì‹œì•„  ë ˆë°”ë…¼  ë§ë ˆì´ì‹œì•„  ëª°ë””ë¸Œ  ëª½ê³¨  ë¯¸ì–€ë§ˆ  ë°”ë ˆì¸  ë°©ê¸€ë¼ë°ì‹œ  ë² íŠ¸ë‚¨  ë¶€íƒ„  ë¸Œë£¨ë‚˜ì´  ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„  ìŠ¤ë¦¬ë‘ì¹´  ì‹œë¦¬ì•„  ì‹±ê°€í¬ë¥´  ì•„ëì—ë¯¸ë¦¬íŠ¸  ì•„ë¥´ë©”ë‹ˆì•„  ì•„ì œë¥´ë°”ì´ì”  ì•„í”„ê°€ë‹ˆìŠ¤íƒ„  ì˜ˆë©˜  ì˜¤ë§Œ  ìš”ë¥´ë‹¨  ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„  ì´ë¼í¬  ì´ë€  ì´ìŠ¤ë¼ì—˜  ì´ì§‘íŠ¸  ì¸ë„  ì¸ë„ë„¤ì‹œì•„  ì¼ë³¸  ì¡°ì„ ë¯¼ì£¼ì£¼ì˜ì¸ë¯¼ê³µí™”êµ­  ì¡°ì§€ì•„  ì¤‘í™”ë¯¼êµ­  ì¤‘í™”ì¸ë¯¼ê³µí™”êµ­  ì¹´ìíìŠ¤íƒ„  ì¹´íƒ€ë¥´  ìº„ë³´ë””ì•„  ì¿ ì›¨ì´íŠ¸  í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„  í‚¤í”„ë¡œìŠ¤  íƒ€ì§€í‚¤ìŠ¤íƒ„  íƒœêµ­  íˆ¬ë¥´í¬ë©”ë‹ˆìŠ¤íƒ„  íŠ€ë¥´í‚¤ì˜ˆ  íŒŒí‚¤ìŠ¤íƒ„  íŒ”ë ˆìŠ¤íƒ€ì¸  í•„ë¦¬í•€ |  |", "score": 0.6894943, "raw_content": null}, {"url": "https://kosis.kr/visual/populationKorea/PopulationDashBoardMain.do", "title": "ì¸êµ¬ìƒí™©íŒ | ì¸êµ¬ë¡œ ë³´ëŠ” ëŒ€í•œë¯¼êµ­ - KOSIS êµ­ê°€í†µê³„í¬í„¸", "content": "ì§€ì—­ë³„ ì¸êµ¬ í†µê³„ì— ëŒ€í•œ í˜„í™© ë° ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê´€ê³„ë„ë§µì„ í†µí•´ ì¸êµ¬ì™€ ì‚¬íšŒÂ·ê²½ì œê°„ ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¸êµ¬ í†µê³„ë¥¼ ë” ë‹¤ì–‘í•œ í˜•íƒœë¡œ ê²½í—˜í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. #### 40ì„¸ 1972ë…„ìƒ ë˜í•œ ì¸êµ¬ êµ¬ì¡°ê°€ ì¶œìƒ, ì‚¬ë§, ì´ë™ ë“± ìš”ì¸ì˜ ì‹œë‚˜ë¦¬ì˜¤ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ìƒì„¸í™”ë©´ì—ì„œ ë” ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. #### 40ì„¸ 1972ë…„ìƒ ë˜í•œ ì¸êµ¬ êµ¬ì¡°ê°€ ì¶œìƒ, ì‚¬ë§, ì´ë™ ë“± ìš”ì¸ì˜ ì‹œë‚˜ë¦¬ì˜¤ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ìƒì„¸í™”ë©´ì—ì„œ ë” ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. #### 40ì„¸ 1972ë…„ìƒ ë˜í•œ ì¸êµ¬ êµ¬ì¡°ê°€ ì¶œìƒ, ì‚¬ë§, ì´ë™ ë“± ìš”ì¸ì˜ ì‹œë‚˜ë¦¬ì˜¤ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ìƒì„¸í™”ë©´ì—ì„œ ë” ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ### ì´ì¸êµ¬ (ì¶”ê³„ ê¸°ì¤€) ### í•™ë ¹ì¸êµ¬ (6-21ì„¸) ### 1ì¸ ê°€êµ¬ ### 1ì¸ ê°€êµ¬ ë¹„ì¤‘ ### ì—°ë ¹ëŒ€ë³„ 1ì¸ ê°€êµ¬ (2025) ### ì¥ë˜ ì¶œìƒì•„ìˆ˜ ### ì¥ë˜ ì‚¬ë§ììˆ˜ ### êµ­ì œìˆœì´ë™ ### ì™¸êµ­ì¸ êµ­ì œìˆœì´ë™ ### ì™¸êµ­ì¸ ì·¨ì—…ì", "score": 0.2750377, "raw_content": null}], "response_time": 0.96}
    ==================================[1m Ai Message [0m==================================
    Name: research_agent
    Tool Calls:
      transfer_to_math_agent (call_j4PhPXMq9VhgG8PqYbvUQz4s)
     Call ID: call_j4PhPXMq9VhgG8PqYbvUQz4s
      Args:
    =================================[1m Tool Message [0m=================================
    Name: transfer_to_math_agent
    
    Successfully transferred to math_agent
    ==================================[1m Ai Message [0m==================================
    Name: math_agent
    Tool Calls:
      multiply (call_zEvaMRfYvYpnVLz768SAzpcO)
     Call ID: call_zEvaMRfYvYpnVLz768SAzpcO
      Args:
        a: 51164582
        b: 2
    =================================[1m Tool Message [0m=================================
    Name: multiply
    
    102329164.0
    ==================================[1m Ai Message [0m==================================
    Name: math_agent
    
    í•œêµ­ì˜ ì¸êµ¬ëŠ” ì•½ 51,164,582ëª…ì´ê³ , ì´ ì¸êµ¬ ìˆ˜ì— 2ë¥¼ ê³±í•˜ë©´ 102,329,164ëª…ì´ ë©ë‹ˆë‹¤.


### **3. ê³„ì¸µì  ì•„í‚¤í…ì²˜ (Supervisor + Swarm ì¡°í•©)** 

- **íŠ¹ì§•**: ìŠˆí¼ë°”ì´ì €ì™€ ìŠ¤ì›œì„ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ ì‘ì—… íë¦„ êµ¬í˜„
- **ì ìš©**: ëŒ€ê·œëª¨ ì‹œìŠ¤í…œì—ì„œ ì—ì´ì „íŠ¸ íŒ€ë“¤ì„ ê´€ë¦¬í•  ë•Œ (ìƒìœ„ ìŠˆí¼ë°”ì´ì €ê°€ í•˜ìœ„ ìŠ¤ì›œë“¤ì„ ê´€ë¦¬í•˜ëŠ” ê³„ì¸µì  ì‹œìŠ¤í…œ)


```python
from langgraph_supervisor import create_supervisor
from langgraph_swarm import create_swarm, create_handoff_tool
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from langchain.chat_models import init_chat_model

# í•˜ìœ„ ìŠ¤ì›œ 1: ì •ë³´ ìˆ˜ì§‘ íŒ€
research_handoff = create_handoff_tool(
    agent_name="data_analyst",
    description="ìƒì„¸í•œ ë°ì´í„° ë¶„ì„ì´ í•„ìš”í•  ë•Œ ë°ì´í„° ë¶„ì„ê°€ì—ê²Œ ì „ë‹¬"
)

basic_researcher = create_react_agent(
    model=init_chat_model("openai:gpt-4.1-mini"),
    tools=[tavily_search, research_handoff],
    prompt="ê¸°ë³¸ ì—°êµ¬ì. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì •ë³´ ìˆ˜ì§‘. ìƒì„¸ ë¶„ì„ì´ í•„ìš”í•˜ë©´ ë°ì´í„° ë¶„ì„ê°€ì—ê²Œ ì „ë‹¬.",
    name="basic_researcher"
)

analyst_handoff = create_handoff_tool(
    agent_name="basic_researcher", 
    description="ê¸°ë³¸ ì •ë³´ ê²€ìƒ‰ì´ í•„ìš”í•  ë•Œ ê¸°ë³¸ ì—°êµ¬ìì—ê²Œ ì „ë‹¬"
)

data_analyst = create_react_agent(
    model=init_chat_model("openai:gpt-4.1-mini"),
    tools=[tavily_search, analyst_handoff],
    prompt="ë°ì´í„° ë¶„ì„ê°€. ìƒì„¸í•œ ë¶„ì„ê³¼ ì¸ì‚¬ì´íŠ¸ ì œê³µ. ê¸°ë³¸ ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ ê¸°ë³¸ ì—°êµ¬ìì—ê²Œ ì „ë‹¬.",
    name="data_analyst"
)

research_swarm = create_swarm(
    agents=[basic_researcher, data_analyst],
    default_active_agent="basic_researcher"
).compile(name="research_swarm")

# í•˜ìœ„ ìŠ¤ì›œ 2: ê³„ì‚° íŒ€  
calc_handoff = create_handoff_tool(
    agent_name="advanced_calculator",
    description="ë³µì¡í•œ ê³„ì‚°ì´ í•„ìš”í•  ë•Œ ê³ ê¸‰ ê³„ì‚°ê¸°ì—ê²Œ ì „ë‹¬"
)

basic_calculator = create_react_agent(
    model=init_chat_model("openai:gpt-4.1-mini"),
    tools=[add, multiply, calc_handoff],
    prompt="ê¸°ë³¸ ê³„ì‚°ê¸°. ê°„ë‹¨í•œ ë§ì…ˆê³¼ ê³±ì…ˆ ìˆ˜í–‰. ë³µì¡í•œ ê³„ì‚°ì€ ê³ ê¸‰ ê³„ì‚°ê¸°ì—ê²Œ ì „ë‹¬.",
    name="basic_calculator"
)

advanced_handoff = create_handoff_tool(
    agent_name="basic_calculator",
    description="ê¸°ë³¸ ê³„ì‚°ì´ í•„ìš”í•  ë•Œ ê¸°ë³¸ ê³„ì‚°ê¸°ì—ê²Œ ì „ë‹¬"
)

advanced_calculator = create_react_agent(
    model=init_chat_model("openai:gpt-4.1-mini"),
    tools=[add, multiply, divide, advanced_handoff],
    prompt="ê³ ê¸‰ ê³„ì‚°ê¸°. ë³µì¡í•œ ê³„ì‚° ìˆ˜í–‰. ê¸°ë³¸ ê³„ì‚°ì€ ê¸°ë³¸ ê³„ì‚°ê¸°ì—ê²Œ ì „ë‹¬.",
    name="advanced_calculator"
)

calc_swarm = create_swarm(
    agents=[basic_calculator, advanced_calculator],
    default_active_agent="basic_calculator"
).compile(name="calc_swarm")

# ìµœìƒìœ„ ìŠˆí¼ë°”ì´ì €
top_supervisor = create_supervisor(
    agents=[research_swarm, calc_swarm],
    model=init_chat_model("openai:gpt-4.1"),
    prompt="""ìµœìƒìœ„ ìŠˆí¼ë°”ì´ì €ì…ë‹ˆë‹¤. ë‘ ê°œì˜ ì „ë¬¸ íŒ€ì„ ê´€ë¦¬í•©ë‹ˆë‹¤:

1. research_swarm: ì •ë³´ ê²€ìƒ‰ê³¼ ë°ì´í„° ë¶„ì„ ë‹´ë‹¹
2. calc_swarm: ìˆ˜í•™ ê³„ì‚° ë‹´ë‹¹

ì‘ì—…ì˜ ì„±ê²©ì— ë”°ë¼ ì ì ˆí•œ íŒ€ì—ê²Œ í• ë‹¹í•˜ì„¸ìš”."""
).compile(name="top_supervisor")


# ê·¸ë˜í”„ ì‹œê°í™”
display(Image(top_supervisor.get_graph(xray=False).draw_mermaid_png()))
```


    
![png](/Users/jussuit/Desktop/temp/data/processed/markdown/day6/DAY06_004_LangGraph_Multi-Agent_21_0.png)
    



```python
# ì‹¤í–‰
result = top_supervisor.invoke({
    "messages": [{
        "role": "user", 
        "content": "í•œêµ­ì˜ ì¸êµ¬ë¥¼ ì°¾ì•„ì„œ, ì¸êµ¬ ìˆ˜ì— 2ë¥¼ ê³±í•´ì£¼ì„¸ìš”."
    }]
})

for m in result["messages"]:
    m.pretty_print()
```

    Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel is_last_step, ignoring it.
    Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel remaining_steps, ignoring it.
    Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel is_last_step, ignoring it.
    Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel remaining_steps, ignoring it.


    ================================[1m Human Message [0m=================================
    
    í•œêµ­ì˜ ì¸êµ¬ë¥¼ ì°¾ì•„ì„œ, ì¸êµ¬ ìˆ˜ì— 2ë¥¼ ê³±í•´ì£¼ì„¸ìš”.
    ==================================[1m Ai Message [0m==================================
    Name: supervisor
    Tool Calls:
      transfer_to_research_swarm (call_xireom4WNo7wL3fL0QlN502P)
     Call ID: call_xireom4WNo7wL3fL0QlN502P
      Args:
    =================================[1m Tool Message [0m=================================
    Name: transfer_to_research_swarm
    
    Successfully transferred to research_swarm
    ==================================[1m Ai Message [0m==================================
    Name: basic_researcher
    
    2025ë…„ 6ì›” ê¸°ì¤€ í•œêµ­ì˜ ì´ ì¸êµ¬ìˆ˜ëŠ” ì•½ 51,164,582ëª…ì…ë‹ˆë‹¤. ì´ë¥¼ 2ë°° í•˜ë©´ 102,329,164ëª…ì…ë‹ˆë‹¤.
    ==================================[1m Ai Message [0m==================================
    Name: research_swarm
    
    Transferring back to supervisor
    Tool Calls:
      transfer_back_to_supervisor (f753c531-768f-4c4d-9750-cc499b5ec465)
     Call ID: f753c531-768f-4c4d-9750-cc499b5ec465
      Args:
    =================================[1m Tool Message [0m=================================
    Name: transfer_back_to_supervisor
    
    Successfully transferred back to supervisor
    ==================================[1m Ai Message [0m==================================
    Name: supervisor
    Tool Calls:
      transfer_to_calc_swarm (call_MX60Vf7yaI7goKuUYUMjqLub)
     Call ID: call_MX60Vf7yaI7goKuUYUMjqLub
      Args:
        korean_population: 51164582
        multiplier: 2
    =================================[1m Tool Message [0m=================================
    Name: transfer_to_calc_swarm
    
    Successfully transferred to calc_swarm
    ==================================[1m Ai Message [0m==================================
    Name: basic_calculator
    
    í•œêµ­ì˜ ì¸êµ¬ ì•½ 51,164,582ëª…ì— 2ë¥¼ ê³±í•˜ë©´ 102,329,164ëª…ì…ë‹ˆë‹¤.
    ==================================[1m Ai Message [0m==================================
    Name: calc_swarm
    
    Transferring back to supervisor
    Tool Calls:
      transfer_back_to_supervisor (2692c18b-5dea-4e4e-96bf-69715c1fa147)
     Call ID: 2692c18b-5dea-4e4e-96bf-69715c1fa147
      Args:
    =================================[1m Tool Message [0m=================================
    Name: transfer_back_to_supervisor
    
    Successfully transferred back to supervisor
    ==================================[1m Ai Message [0m==================================
    Name: supervisor
    
    2025ë…„ 6ì›” ê¸°ì¤€ í•œêµ­ì˜ ì¸êµ¬ëŠ” ì•½ 51,164,582ëª…ì…ë‹ˆë‹¤.  
    ì´ ìˆ˜ì— 2ë¥¼ ê³±í•˜ë©´ 102,329,164ëª…ì…ë‹ˆë‹¤.



```python

```
