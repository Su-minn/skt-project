{"title": "Guides¶", "content": "Get started\n\nGuides\n    \n  \n\n\n\n\n\n\n\n\n            Guides\n          \n\n\n\n\n\n    Agent development\n    \n  \n\n\n\n\n\n            Agent development\n          \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n\n    Run an agent\n    \n  \n\n\n\n\n\n\n\n\n\n    LangGraph APIs\n    \n  \n\n\n\n\n\n            LangGraph APIs\n          \n\n\n\n\n    Graph API\n    \n  \n\n\n\n\n\n\n    Functional API\n    \n  \n\n\n\n\n\n\n    Runtime\n    \n  \n\n\n\n\n\n\n\n\n\n    Core capabilities\n    \n  \n\n\n\n\n\n            Core capabilities\n          \n\n\n\n\n    Streaming\n    \n  \n\n\n\n\n\n\n    Persistence\n    \n  \n\n\n\n\n\n\n    Durable execution\n    \n  \n\n\n\n\n\n\n    Memory\n    \n  \n\n\n\n\n\n\n    Context\n    \n  \n\n\n\n\n\n\n    Models\n    \n  \n\n\n\n\n\n\n    Tools\n    \n  \n\n\n\n\n\n\n    Human-in-the-loop\n    \n  \n\n\n\n\n\n\n    Time travel\n    \n  \n\n\n\n\n\n\n    Subgraphs\n    \n  \n\n\n\n\n\n\n    Multi-agent\n    \n  \n\n\n\n\n\n\n    MCP\n    \n  \n\n\n\n\n\n\n    Tracing\n    \n  \n\n\n\n\n\n\n\n\n\n\n    Platform-only capabilities\n    \n  \n\n\n\n\n\n            Platform-only capabilities\n          \n\n\n\n\n    LangGraph Platform\n    \n  \n\n\n\n\n\n\n    Authentication & access control\n    \n  \n\n\n\n\n\n\n    Assistants\n    \n  \n\n\n\n\n\n\n    Double-texting\n    \n  \n\n\n\n\n\n\n    Webhooks\n    \n  \n\n\n\n\n\n\n    Cron jobs\n    \n  \n\n\n\n\n\n\n    Server customization\n    \n  \n\n\n\n\n\n\n    Data management\n    \n  \n\n\n\n\n\n\n    Deployment\n\nAgent development\n    \n  \n\n\n\n\n\n            Agent development\n          \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n\n    Run an agent\n\nOverview\n\nRun an agent\n\nLangGraph APIs\n    \n  \n\n\n\n\n\n            LangGraph APIs\n          \n\n\n\n\n    Graph API\n    \n  \n\n\n\n\n\n\n    Functional API\n    \n  \n\n\n\n\n\n\n    Runtime\n\nGraph API\n\nFunctional API\n\nRuntime\n\nCore capabilities\n    \n  \n\n\n\n\n\n            Core capabilities\n          \n\n\n\n\n    Streaming\n    \n  \n\n\n\n\n\n\n    Persistence\n    \n  \n\n\n\n\n\n\n    Durable execution\n    \n  \n\n\n\n\n\n\n    Memory\n    \n  \n\n\n\n\n\n\n    Context\n    \n  \n\n\n\n\n\n\n    Models\n    \n  \n\n\n\n\n\n\n    Tools\n    \n  \n\n\n\n\n\n\n    Human-in-the-loop\n    \n  \n\n\n\n\n\n\n    Time travel\n    \n  \n\n\n\n\n\n\n    Subgraphs\n    \n  \n\n\n\n\n\n\n    Multi-agent\n    \n  \n\n\n\n\n\n\n    MCP\n    \n  \n\n\n\n\n\n\n    Tracing\n\nStreaming\n\nPersistence\n\nDurable execution\n\nMemory\n\nContext\n\nModels\n\nTools\n\nHuman-in-the-loop\n\nTime travel\n\nSubgraphs\n\nMulti-agent\n\nMCP\n\nTracing\n\nPlatform-only capabilities\n    \n  \n\n\n\n\n\n            Platform-only capabilities\n          \n\n\n\n\n    LangGraph Platform\n    \n  \n\n\n\n\n\n\n    Authentication & access control\n    \n  \n\n\n\n\n\n\n    Assistants\n    \n  \n\n\n\n\n\n\n    Double-texting\n    \n  \n\n\n\n\n\n\n    Webhooks\n    \n  \n\n\n\n\n\n\n    Cron jobs\n    \n  \n\n\n\n\n\n\n    Server customization\n    \n  \n\n\n\n\n\n\n    Data management\n    \n  \n\n\n\n\n\n\n    Deployment\n\nLangGraph Platform\n\nAuthentication & access control\n\nAssistants\n\nDouble-texting\n\nWebhooks\n\nCron jobs\n\nServer customization\n\nData management\n\nDeployment\n\nReference\n\nExamples\n\nAdditional resources\n\nAgent development\n\nLangGraph APIs\n\nCore capabilities\n\nPlatform-only capabilities\n\n\n\n## Guides¶\n\nThe pages in this section provide a conceptual overview and how-tos for the following topics:\n\n\n\n## Agent development¶\n\nOverview: Use prebuilt components to build an agent.\n\nRun an agent: Run an agent by providing input, interpreting output, enabling streaming, and controlling execution limits.\n\n\n\n## LangGraph APIs¶\n\nGraph API: Use the Graph API to define workflows using a graph paradigm.\n\nFunctional API: Use Functional API to build workflows using a functional paradigm without thinking about the graph structure.\n\nRuntime: Pregel implements LangGraph's runtime, managing the execution of LangGraph applications.\n\n\n\n## Core capabilities¶\n\nThese capabilities are available in both LangGraph OSS and the LangGraph Platform.\n\nStreaming: Stream outputs from a LangGraph graph.\n\nPersistence: Persist the state of a LangGraph graph.\n\nDurable execution: Save progress at key points in the graph execution.\n\nMemory: Remember information about previous interactions.\n\nContext: Pass outside data to a LangGraph graph to provide context for the graph execution.\n\nModels: Integrate various LLMs into your LangGraph application.\n\nTools: Interface directly with external systems.\n\nHuman-in-the-loop: Pause a graph and wait for human input at any point in a workflow.\n\nTime travel: Travel back in time to a specific point in the execution of a LangGraph graph.\n\nSubgraphs: Build modular graphs.\n\nMulti-agent: Break down a complex workflow into multiple agents.\n\nMCP: Use MCP servers in a LangGraph graph.\n\nEvaluation: Use LangSmith to evaluate your graph's performance.\n\n\n\n## Platform-only capabilities¶\n\nThese capabilities are only available in LangGraph Platform.\n\nAuthentication and access control: Authenticate and authorize users to access a LangGraph graph.\n\nAssistants: Build assistants that can be used to interact with a LangGraph graph.\n\nDouble-texting: Handle double-texting (consecutive messages before a first response is returned) in a LangGraph graph.\n\nWebhooks: Send webhooks to a LangGraph graph.\n\nCron jobs: Schedule jobs to run at a specific time.\n\nServer customization: Customize the server that runs a LangGraph graph.\n\nData management: Manage data in a LangGraph graph.\n\nDeployment: Deploy a LangGraph graph to a server.", "url": "https://langchain-ai.github.io/langgraph/guides/", "path": "/langgraph/guides/", "domain": "langchain-ai.github.io", "source": "LangGraph 공식문서", "content_length": 5027, "crawled_at": "2025-07-18 03:24:33"}
{"title": "Graph API concepts¶", "content": "Get started\n\nGuides\n    \n  \n\n\n\n\n\n\n\n\n            Guides\n          \n\n\n\n\n\n    Agent development\n    \n  \n\n\n\n\n\n            Agent development\n          \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n\n    Run an agent\n    \n  \n\n\n\n\n\n\n\n\n\n    LangGraph APIs\n    \n  \n\n\n\n\n\n            LangGraph APIs\n          \n\n\n\n\n\n    Graph API\n    \n  \n\n\n\n\n\n            Graph API\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Graphs\n    \n\n\n\n\n\n\n      StateGraph\n    \n\n\n\n\n\n      Compiling your graph\n    \n\n\n\n\n\n\n\n\n      State\n    \n\n\n\n\n\n\n      Schema\n    \n\n\n\n\n\n\n      Multiple schemas\n    \n\n\n\n\n\n\n\n\n      Reducers\n    \n\n\n\n\n\n\n      Default Reducer\n    \n\n\n\n\n\n\n\n\n      Working with Messages in Graph State\n    \n\n\n\n\n\n\n      Why use messages?\n    \n\n\n\n\n\n      Using Messages in your Graph\n    \n\n\n\n\n\n      Serialization\n    \n\n\n\n\n\n      MessagesState\n    \n\n\n\n\n\n\n\n\n\n\n\n      Nodes\n    \n\n\n\n\n\n\n      START Node\n    \n\n\n\n\n\n      END Node\n    \n\n\n\n\n\n      Node Caching\n    \n\n\n\n\n\n\n\n\n      Edges\n    \n\n\n\n\n\n\n      Normal Edges\n    \n\n\n\n\n\n      Conditional Edges\n    \n\n\n\n\n\n      Entry Point\n    \n\n\n\n\n\n      Conditional Entry Point\n    \n\n\n\n\n\n\n\n\n      Send\n    \n\n\n\n\n\n      Command\n    \n\n\n\n\n\n\n      When should I use Command instead of conditional edges?\n    \n\n\n\n\n\n      Navigating to a node in a parent graph\n    \n\n\n\n\n\n      Using inside tools\n    \n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n\n\n\n      Graph Migrations\n    \n\n\n\n\n\n      Configuration\n    \n\n\n\n\n\n\n      Recursion Limit\n    \n\n\n\n\n\n\n\n\n      Visualization\n    \n\n\n\n\n\n\n\n\n    Use the Graph API\n    \n  \n\n\n\n\n\n\n\n\n    Functional API\n    \n  \n\n\n\n\n\n\n    Runtime\n    \n  \n\n\n\n\n\n\n\n\n\n    Core capabilities\n    \n  \n\n\n\n\n\n            Core capabilities\n          \n\n\n\n\n    Streaming\n    \n  \n\n\n\n\n\n\n    Persistence\n    \n  \n\n\n\n\n\n\n    Durable execution\n    \n  \n\n\n\n\n\n\n    Memory\n    \n  \n\n\n\n\n\n\n    Context\n    \n  \n\n\n\n\n\n\n    Models\n    \n  \n\n\n\n\n\n\n    Tools\n    \n  \n\n\n\n\n\n\n    Human-in-the-loop\n    \n  \n\n\n\n\n\n\n    Time travel\n    \n  \n\n\n\n\n\n\n    Subgraphs\n    \n  \n\n\n\n\n\n\n    Multi-agent\n    \n  \n\n\n\n\n\n\n    MCP\n    \n  \n\n\n\n\n\n\n    Tracing\n    \n  \n\n\n\n\n\n\n\n\n\n\n    Platform-only capabilities\n    \n  \n\n\n\n\n\n            Platform-only capabilities\n          \n\n\n\n\n    LangGraph Platform\n    \n  \n\n\n\n\n\n\n    Authentication & access control\n    \n  \n\n\n\n\n\n\n    Assistants\n    \n  \n\n\n\n\n\n\n    Double-texting\n    \n  \n\n\n\n\n\n\n    Webhooks\n    \n  \n\n\n\n\n\n\n    Cron jobs\n    \n  \n\n\n\n\n\n\n    Server customization\n    \n  \n\n\n\n\n\n\n    Data management\n    \n  \n\n\n\n\n\n\n    Deployment\n\nAgent development\n    \n  \n\n\n\n\n\n            Agent development\n          \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n\n    Run an agent\n\nOverview\n\nRun an agent\n\nLangGraph APIs\n    \n  \n\n\n\n\n\n            LangGraph APIs\n          \n\n\n\n\n\n    Graph API\n    \n  \n\n\n\n\n\n            Graph API\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Graphs\n    \n\n\n\n\n\n\n      StateGraph\n    \n\n\n\n\n\n      Compiling your graph\n    \n\n\n\n\n\n\n\n\n      State\n    \n\n\n\n\n\n\n      Schema\n    \n\n\n\n\n\n\n      Multiple schemas\n    \n\n\n\n\n\n\n\n\n      Reducers\n    \n\n\n\n\n\n\n      Default Reducer\n    \n\n\n\n\n\n\n\n\n      Working with Messages in Graph State\n    \n\n\n\n\n\n\n      Why use messages?\n    \n\n\n\n\n\n      Using Messages in your Graph\n    \n\n\n\n\n\n      Serialization\n    \n\n\n\n\n\n      MessagesState\n    \n\n\n\n\n\n\n\n\n\n\n\n      Nodes\n    \n\n\n\n\n\n\n      START Node\n    \n\n\n\n\n\n      END Node\n    \n\n\n\n\n\n      Node Caching\n    \n\n\n\n\n\n\n\n\n      Edges\n    \n\n\n\n\n\n\n      Normal Edges\n    \n\n\n\n\n\n      Conditional Edges\n    \n\n\n\n\n\n      Entry Point\n    \n\n\n\n\n\n      Conditional Entry Point\n    \n\n\n\n\n\n\n\n\n      Send\n    \n\n\n\n\n\n      Command\n    \n\n\n\n\n\n\n      When should I use Command instead of conditional edges?\n    \n\n\n\n\n\n      Navigating to a node in a parent graph\n    \n\n\n\n\n\n      Using inside tools\n    \n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n\n\n\n      Graph Migrations\n    \n\n\n\n\n\n      Configuration\n    \n\n\n\n\n\n\n      Recursion Limit\n    \n\n\n\n\n\n\n\n\n      Visualization\n    \n\n\n\n\n\n\n\n\n    Use the Graph API\n    \n  \n\n\n\n\n\n\n\n\n    Functional API\n    \n  \n\n\n\n\n\n\n    Runtime\n\nGraph API\n    \n  \n\n\n\n\n\n            Graph API\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Graphs\n    \n\n\n\n\n\n\n      StateGraph\n    \n\n\n\n\n\n      Compiling your graph\n    \n\n\n\n\n\n\n\n\n      State\n    \n\n\n\n\n\n\n      Schema\n    \n\n\n\n\n\n\n      Multiple schemas\n    \n\n\n\n\n\n\n\n\n      Reducers\n    \n\n\n\n\n\n\n      Default Reducer\n    \n\n\n\n\n\n\n\n\n      Working with Messages in Graph State\n    \n\n\n\n\n\n\n      Why use messages?\n    \n\n\n\n\n\n      Using Messages in your Graph\n    \n\n\n\n\n\n      Serialization\n    \n\n\n\n\n\n      MessagesState\n    \n\n\n\n\n\n\n\n\n\n\n\n      Nodes\n    \n\n\n\n\n\n\n      START Node\n    \n\n\n\n\n\n      END Node\n    \n\n\n\n\n\n      Node Caching\n    \n\n\n\n\n\n\n\n\n      Edges\n    \n\n\n\n\n\n\n      Normal Edges\n    \n\n\n\n\n\n      Conditional Edges\n    \n\n\n\n\n\n      Entry Point\n    \n\n\n\n\n\n      Conditional Entry Point\n    \n\n\n\n\n\n\n\n\n      Send\n    \n\n\n\n\n\n      Command\n    \n\n\n\n\n\n\n      When should I use Command instead of conditional edges?\n    \n\n\n\n\n\n      Navigating to a node in a parent graph\n    \n\n\n\n\n\n      Using inside tools\n    \n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n\n\n\n      Graph Migrations\n    \n\n\n\n\n\n      Configuration\n    \n\n\n\n\n\n\n      Recursion Limit\n    \n\n\n\n\n\n\n\n\n      Visualization\n    \n\n\n\n\n\n\n\n\n    Use the Graph API\n\nOverview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Graphs\n    \n\n\n\n\n\n\n      StateGraph\n    \n\n\n\n\n\n      Compiling your graph\n    \n\n\n\n\n\n\n\n\n      State\n    \n\n\n\n\n\n\n      Schema\n    \n\n\n\n\n\n\n      Multiple schemas\n    \n\n\n\n\n\n\n\n\n      Reducers\n    \n\n\n\n\n\n\n      Default Reducer\n    \n\n\n\n\n\n\n\n\n      Working with Messages in Graph State\n    \n\n\n\n\n\n\n      Why use messages?\n    \n\n\n\n\n\n      Using Messages in your Graph\n    \n\n\n\n\n\n      Serialization\n    \n\n\n\n\n\n      MessagesState\n    \n\n\n\n\n\n\n\n\n\n\n\n      Nodes\n    \n\n\n\n\n\n\n      START Node\n    \n\n\n\n\n\n      END Node\n    \n\n\n\n\n\n      Node Caching\n    \n\n\n\n\n\n\n\n\n      Edges\n    \n\n\n\n\n\n\n      Normal Edges\n    \n\n\n\n\n\n      Conditional Edges\n    \n\n\n\n\n\n      Entry Point\n    \n\n\n\n\n\n      Conditional Entry Point\n    \n\n\n\n\n\n\n\n\n      Send\n    \n\n\n\n\n\n      Command\n    \n\n\n\n\n\n\n      When should I use Command instead of conditional edges?\n    \n\n\n\n\n\n      Navigating to a node in a parent graph\n    \n\n\n\n\n\n      Using inside tools\n    \n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n\n\n\n      Graph Migrations\n    \n\n\n\n\n\n      Configuration\n    \n\n\n\n\n\n\n      Recursion Limit\n    \n\n\n\n\n\n\n\n\n      Visualization\n\nGraphs\n    \n\n\n\n\n\n\n      StateGraph\n    \n\n\n\n\n\n      Compiling your graph\n\nStateGraph\n\nCompiling your graph\n\nState\n    \n\n\n\n\n\n\n      Schema\n    \n\n\n\n\n\n\n      Multiple schemas\n    \n\n\n\n\n\n\n\n\n      Reducers\n    \n\n\n\n\n\n\n      Default Reducer\n    \n\n\n\n\n\n\n\n\n      Working with Messages in Graph State\n    \n\n\n\n\n\n\n      Why use messages?\n    \n\n\n\n\n\n      Using Messages in your Graph\n    \n\n\n\n\n\n      Serialization\n    \n\n\n\n\n\n      MessagesState\n\nSchema\n    \n\n\n\n\n\n\n      Multiple schemas\n\nMultiple schemas\n\nReducers\n    \n\n\n\n\n\n\n      Default Reducer\n\nDefault Reducer\n\nWorking with Messages in Graph State\n    \n\n\n\n\n\n\n      Why use messages?\n    \n\n\n\n\n\n      Using Messages in your Graph\n    \n\n\n\n\n\n      Serialization\n    \n\n\n\n\n\n      MessagesState\n\nWhy use messages?\n\nUsing Messages in your Graph\n\nSerialization\n\nMessagesState\n\nNodes\n    \n\n\n\n\n\n\n      START Node\n    \n\n\n\n\n\n      END Node\n    \n\n\n\n\n\n      Node Caching\n\nSTART Node\n\nEND Node\n\nNode Caching\n\nEdges\n    \n\n\n\n\n\n\n      Normal Edges\n    \n\n\n\n\n\n      Conditional Edges\n    \n\n\n\n\n\n      Entry Point\n    \n\n\n\n\n\n      Conditional Entry Point\n\nNormal Edges\n\nConditional Edges\n\nEntry Point\n\nConditional Entry Point\n\nSend\n\nCommand\n    \n\n\n\n\n\n\n      When should I use Command instead of conditional edges?\n    \n\n\n\n\n\n      Navigating to a node in a parent graph\n    \n\n\n\n\n\n      Using inside tools\n    \n\n\n\n\n\n      Human-in-the-loop\n\nWhen should I use Command instead of conditional edges?\n\nNavigating to a node in a parent graph\n\nUsing inside tools\n\nHuman-in-the-loop\n\nGraph Migrations\n\nConfiguration\n    \n\n\n\n\n\n\n      Recursion Limit\n\nRecursion Limit\n\nVisualization\n\nUse the Graph API\n\nFunctional API\n\nRuntime\n\nCore capabilities\n    \n  \n\n\n\n\n\n            Core capabilities\n          \n\n\n\n\n    Streaming\n    \n  \n\n\n\n\n\n\n    Persistence\n    \n  \n\n\n\n\n\n\n    Durable execution\n    \n  \n\n\n\n\n\n\n    Memory\n    \n  \n\n\n\n\n\n\n    Context\n    \n  \n\n\n\n\n\n\n    Models\n    \n  \n\n\n\n\n\n\n    Tools\n    \n  \n\n\n\n\n\n\n    Human-in-the-loop\n    \n  \n\n\n\n\n\n\n    Time travel\n    \n  \n\n\n\n\n\n\n    Subgraphs\n    \n  \n\n\n\n\n\n\n    Multi-agent\n    \n  \n\n\n\n\n\n\n    MCP\n    \n  \n\n\n\n\n\n\n    Tracing\n\nStreaming\n\nPersistence\n\nDurable execution\n\nMemory\n\nContext\n\nModels\n\nTools\n\nHuman-in-the-loop\n\nTime travel\n\nSubgraphs\n\nMulti-agent\n\nMCP\n\nTracing\n\nPlatform-only capabilities\n    \n  \n\n\n\n\n\n            Platform-only capabilities\n          \n\n\n\n\n    LangGraph Platform\n    \n  \n\n\n\n\n\n\n    Authentication & access control\n    \n  \n\n\n\n\n\n\n    Assistants\n    \n  \n\n\n\n\n\n\n    Double-texting\n    \n  \n\n\n\n\n\n\n    Webhooks\n    \n  \n\n\n\n\n\n\n    Cron jobs\n    \n  \n\n\n\n\n\n\n    Server customization\n    \n  \n\n\n\n\n\n\n    Data management\n    \n  \n\n\n\n\n\n\n    Deployment\n\nLangGraph Platform\n\nAuthentication & access control\n\nAssistants\n\nDouble-texting\n\nWebhooks\n\nCron jobs\n\nServer customization\n\nData management\n\nDeployment\n\nReference\n\nExamples\n\nAdditional resources\n\nGraphs\n    \n\n\n\n\n\n\n      StateGraph\n    \n\n\n\n\n\n      Compiling your graph\n\nStateGraph\n\nCompiling your graph\n\nState\n    \n\n\n\n\n\n\n      Schema\n    \n\n\n\n\n\n\n      Multiple schemas\n    \n\n\n\n\n\n\n\n\n      Reducers\n    \n\n\n\n\n\n\n      Default Reducer\n    \n\n\n\n\n\n\n\n\n      Working with Messages in Graph State\n    \n\n\n\n\n\n\n      Why use messages?\n    \n\n\n\n\n\n      Using Messages in your Graph\n    \n\n\n\n\n\n      Serialization\n    \n\n\n\n\n\n      MessagesState\n\nSchema\n    \n\n\n\n\n\n\n      Multiple schemas\n\nMultiple schemas\n\nReducers\n    \n\n\n\n\n\n\n      Default Reducer\n\nDefault Reducer\n\nWorking with Messages in Graph State\n    \n\n\n\n\n\n\n      Why use messages?\n    \n\n\n\n\n\n      Using Messages in your Graph\n    \n\n\n\n\n\n      Serialization\n    \n\n\n\n\n\n      MessagesState\n\nWhy use messages?\n\nUsing Messages in your Graph\n\nSerialization\n\nMessagesState\n\nNodes\n    \n\n\n\n\n\n\n      START Node\n    \n\n\n\n\n\n      END Node\n    \n\n\n\n\n\n      Node Caching\n\nSTART Node\n\nEND Node\n\nNode Caching\n\nEdges\n    \n\n\n\n\n\n\n      Normal Edges\n    \n\n\n\n\n\n      Conditional Edges\n    \n\n\n\n\n\n      Entry Point\n    \n\n\n\n\n\n      Conditional Entry Point\n\nNormal Edges\n\nConditional Edges\n\nEntry Point\n\nConditional Entry Point\n\nSend\n\nCommand\n    \n\n\n\n\n\n\n      When should I use Command instead of conditional edges?\n    \n\n\n\n\n\n      Navigating to a node in a parent graph\n    \n\n\n\n\n\n      Using inside tools\n    \n\n\n\n\n\n      Human-in-the-loop\n\nWhen should I use Command instead of conditional edges?\n\nNavigating to a node in a parent graph\n\nUsing inside tools\n\nHuman-in-the-loop\n\nGraph Migrations\n\nConfiguration\n    \n\n\n\n\n\n\n      Recursion Limit\n\nRecursion Limit\n\nVisualization\n\n\n\n## Graph API concepts¶\n\n\n\n## Graphs¶\n\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\n\nState: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.\n\nState: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.\n\n\n```\nState\n```\n\n```\nTypedDict\n```\n\n```\nBaseModel\n```\nNodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.\n\nNodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.\n\n\n```\nNodes\n```\n\n```\nState\n```\n\n```\nState\n```\nEdges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.\n\nEdges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.\n\n\n```\nEdges\n```\n\n```\nNode\n```\n\n```\nState\n```\nBy composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code.\n\n\n```\nNodes\n```\n\n```\nEdges\n```\n\n```\nState\n```\n\n```\nState\n```\n\n```\nNodes\n```\n\n```\nEdges\n```\nIn short: nodes do the work, edges tell what to do next.\n\nLangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete \"super-steps.\"\n\nA super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or \"channels\"). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.\n\n\n```\ninactive\n```\n\n```\nactive\n```\n\n```\nhalt\n```\n\n```\ninactive\n```\n\n```\ninactive\n```\n\n\n## StateGraph¶\n\nThe StateGraph class is the main graph class to use. This is parameterized by a user defined State object.\n\n\n```\nStateGraph\n```\n\n```\nState\n```\n\n\n## Compiling your graph¶\n\nTo build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?\n\nCompiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method:\n\n\n```\n.compile\n```\n\n```\ngraph = graph_builder.compile(...)\n```\n\n```\ngraph = graph_builder.compile(...)\n```\nYou MUST compile your graph before you can use it.\n\n\n\n## State¶\n\nThe first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.\n\n\n```\nState\n```\n\n```\nState\n```\n\n```\nreducer\n```\n\n```\nState\n```\n\n```\nNodes\n```\n\n```\nEdges\n```\n\n```\nTypedDict\n```\n\n```\nPydantic\n```\n\n```\nNodes\n```\n\n```\nState\n```\n\n```\nreducer\n```\n\n\n## Schema¶\n\nThe main documented way to specify the schema of a graph is by using a TypedDict. If you want to provide default values in your state, use a dataclass. We also support using a Pydantic BaseModel as your graph state if you want recursive data validation (though note that pydantic is less performant than a TypedDict or dataclass).\n\n\n```\nTypedDict\n```\n\n```\ndataclass\n```\n\n```\nTypedDict\n```\n\n```\ndataclass\n```\nBy default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the guide here for how to use.\n\n\n\n## Multiple schemas¶\n\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:\n\nInternal nodes can pass information that is not required in the graph's input / output.\n\nWe may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\n\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState. See this guide for more detail.\n\n\n```\nPrivateState\n```\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an \"internal\" schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the \"internal\" schema to constrain the input and output of the graph. See this guide for more detail.\n\n\n```\ninput\n```\n\n```\noutput\n```\nLet's look at an example:\n\n\n```\nclass InputState(TypedDict):\n    user_input: str\n\nclass OutputState(TypedDict):\n    graph_output: str\n\nclass OverallState(TypedDict):\n    foo: str\n    user_input: str\n    graph_output: str\n\nclass PrivateState(TypedDict):\n    bar: str\n\ndef node_1(state: InputState) -> OverallState:\n    # Write to OverallState\n    return {\"foo\": state[\"user_input\"] + \" name\"}\n\ndef node_2(state: OverallState) -> PrivateState:\n    # Read from OverallState, write to PrivateState\n    return {\"bar\": state[\"foo\"] + \" is\"}\n\ndef node_3(state: PrivateState) -> OutputState:\n    # Read from PrivateState, write to OutputState\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\n\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_2\", \"node_3\")\nbuilder.add_edge(\"node_3\", END)\n\ngraph = builder.compile()\ngraph.invoke({\"user_input\":\"My\"})\n{'graph_output': 'My name is Lance'}\n```\n\n```\nclass InputState(TypedDict):\n    user_input: str\n\nclass OutputState(TypedDict):\n    graph_output: str\n\nclass OverallState(TypedDict):\n    foo: str\n    user_input: str\n    graph_output: str\n\nclass PrivateState(TypedDict):\n    bar: str\n\ndef node_1(state: InputState) -> OverallState:\n    # Write to OverallState\n    return {\"foo\": state[\"user_input\"] + \" name\"}\n\ndef node_2(state: OverallState) -> PrivateState:\n    # Read from OverallState, write to PrivateState\n    return {\"bar\": state[\"foo\"] + \" is\"}\n\ndef node_3(state: PrivateState) -> OutputState:\n    # Read from PrivateState, write to OutputState\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\n\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_2\", \"node_3\")\nbuilder.add_edge(\"node_3\", END)\n\ngraph = builder.compile()\ngraph.invoke({\"user_input\":\"My\"})\n{'graph_output': 'My name is Lance'}\n```\nThere are two subtle and important points to note here:\n\nWe pass state: InputState as the input schema to node_1. But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.\n\nWe pass state: InputState as the input schema to node_1. But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.\n\n\n```\nstate: InputState\n```\n\n```\nnode_1\n```\n\n```\nfoo\n```\n\n```\nOverallState\n```\n\n```\nOverallState\n```\n\n```\nInputState\n```\n\n```\nOutputState\n```\nWe initialize the graph with StateGraph(OverallState,input_schema=InputState,output_schema=OutputState). So, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.\n\nWe initialize the graph with StateGraph(OverallState,input_schema=InputState,output_schema=OutputState). So, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.\n\n\n```\nStateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\n```\n\n```\nPrivateState\n```\n\n```\nnode_2\n```\n\n```\nStateGraph\n```\n\n```\nPrivateState\n```\n\n```\nbar\n```\n\n\n## Reducers¶\n\nReducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\n\n\n```\nState\n```\n\n```\nState\n```\n\n\n## Default Reducer¶\n\nThese two examples show how to use the default reducer:\n\nExample A:\n\n\n```\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: int\n    bar: list[str]\n```\n\n```\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: int\n    bar: list[str]\n```\nIn this example, no reducer functions are specified for any key. Let's assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Let's then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"bye\"]}\n\n\n```\n{\"foo\": 1, \"bar\": [\"hi\"]}\n```\n\n```\nNode\n```\n\n```\n{\"foo\": 2}\n```\n\n```\nNode\n```\n\n```\nState\n```\n\n```\nState\n```\n\n```\n{\"foo\": 2, \"bar\": [\"hi\"]}\n```\n\n```\n{\"bar\": [\"bye\"]}\n```\n\n```\nState\n```\n\n```\n{\"foo\": 2, \"bar\": [\"bye\"]}\n```\nExample B:\n\n\n```\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n```\n\n```\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n```\nIn this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Let's then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}. Notice here that the bar key is updated by adding the two lists together.\n\n\n```\nAnnotated\n```\n\n```\noperator.add\n```\n\n```\nbar\n```\n\n```\n{\"foo\": 1, \"bar\": [\"hi\"]}\n```\n\n```\nNode\n```\n\n```\n{\"foo\": 2}\n```\n\n```\nNode\n```\n\n```\nState\n```\n\n```\nState\n```\n\n```\n{\"foo\": 2, \"bar\": [\"hi\"]}\n```\n\n```\n{\"bar\": [\"bye\"]}\n```\n\n```\nState\n```\n\n```\n{\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}\n```\n\n```\nbar\n```\n\n\n## Working with Messages in Graph State¶\n\n\n\n## Why use messages?¶\n\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide.\n\n\n```\nChatModel\n```\n\n```\nMessage\n```\n\n```\nHumanMessage\n```\n\n```\nAIMessage\n```\n\n\n## Using Messages in your Graph¶\n\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer.\n\n\n```\nMessage\n```\n\n```\nmessages\n```\n\n```\nMessage\n```\n\n```\noperator.add\n```\nHowever, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\n\n\n```\noperator.add\n```\n\n```\nadd_messages\n```\n\n\n## Serialization¶\n\nIn addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:\n\n\n```\nadd_messages\n```\n\n```\nMessage\n```\n\n```\nmessages\n```\n\n```\n# this is supported\n{\"messages\": [HumanMessage(content=\"message\")]}\n\n# and this is also supported\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\n```\n\n```\n# this is supported\n{\"messages\": [HumanMessage(content=\"message\")]}\n\n# and this is also supported\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\n```\nSince the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[\"messages\"][-1].content. Below is an example of a graph that uses add_messages as its reducer function.\n\n\n```\nMessages\n```\n\n```\nadd_messages\n```\n\n```\nstate[\"messages\"][-1].content\n```\n\n```\nadd_messages\n```\nAPI Reference: AnyMessage | add_messages\n\n\n```\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph.message import add_messages\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nclass GraphState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n```\n\n```\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph.message import add_messages\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nclass GraphState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n```\n\n\n## MessagesState¶\n\nSince having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\n\n\n```\nMessagesState\n```\n\n```\nMessagesState\n```\n\n```\nmessages\n```\n\n```\nAnyMessage\n```\n\n```\nadd_messages\n```\n\n```\nfrom langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    documents: list[str]\n```\n\n```\nfrom langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    documents: list[str]\n```\n\n\n## Nodes¶\n\nIn LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a \"config\", containing optional configurable parameters (such as a thread_id).\n\n\n```\nthread_id\n```\nSimilar to NetworkX, you add these nodes to a graph using the add_node method:\n\n\n```\nNetworkX\n```\nAPI Reference: RunnableConfig | StateGraph\n\n\n```\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\n\nclass State(TypedDict):\n    input: str\n    results: str\n\nbuilder = StateGraph(State)\n\n\ndef my_node(state: State, config: RunnableConfig):\n    print(\"In node: \", config[\"configurable\"][\"user_id\"])\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\n\n# The second argument is optional\ndef my_other_node(state: State):\n    return state\n\n\nbuilder.add_node(\"my_node\", my_node)\nbuilder.add_node(\"other_node\", my_other_node)\n...\n```\n\n```\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\n\nclass State(TypedDict):\n    input: str\n    results: str\n\nbuilder = StateGraph(State)\n\n\ndef my_node(state: State, config: RunnableConfig):\n    print(\"In node: \", config[\"configurable\"][\"user_id\"])\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\n\n# The second argument is optional\ndef my_other_node(state: State):\n    return state\n\n\nbuilder.add_node(\"my_node\", my_node)\nbuilder.add_node(\"other_node\", my_other_node)\n...\n```\nBehind the scenes, functions are converted to RunnableLambdas, which add batch and async support to your function, along with native tracing and debugging.\n\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\n\n\n```\nbuilder.add_node(my_node)\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\n```\n\n```\nbuilder.add_node(my_node)\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\n```\n\n\n## START Node¶\n\n\n```\nSTART\n```\nThe START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\n\n\n```\nSTART\n```\nAPI Reference: START\n\n\n```\nfrom langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n```\n\n```\nfrom langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n```\n\n\n## END Node¶\n\n\n```\nEND\n```\nThe END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\n\n\n```\nEND\n```\n\n```\nfrom langgraph.graph import END\n\ngraph.add_edge(\"node_a\", END)\n```\n\n```\nfrom langgraph.graph import END\n\ngraph.add_edge(\"node_a\", END)\n```\n\n\n## Node Caching¶\n\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\n\nSpecify a cache when compiling a graph (or specifying an entrypoint)\n\nSpecify a cache policy for nodes. Each cache policy supports:\nkey_func used to generate a cache key based on the input to a node, which defaults to a hash of the input with pickle.\nttl, the time to live for the cache in seconds. If not specified, the cache will never expire.\n\nkey_func used to generate a cache key based on the input to a node, which defaults to a hash of the input with pickle.\n\n\n```\nkey_func\n```\n\n```\nhash\n```\nttl, the time to live for the cache in seconds. If not specified, the cache will never expire.\n\n\n```\nttl\n```\nFor example:\n\nAPI Reference: StateGraph\n\n\n```\nimport time\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph\nfrom langgraph.cache.memory import InMemoryCache\nfrom langgraph.types import CachePolicy\n\n\nclass State(TypedDict):\n    x: int\n    result: int\n\n\nbuilder = StateGraph(State)\n\n\ndef expensive_node(state: State) -> dict[str, int]:\n    # expensive computation\n    time.sleep(2)\n    return {\"result\": state[\"x\"] * 2}\n\n\nbuilder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\nbuilder.set_entry_point(\"expensive_node\")\nbuilder.set_finish_point(\"expensive_node\")\n\ngraph = builder.compile(cache=InMemoryCache())\n\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))  # (1)!\n[{'expensive_node': {'result': 10}}]\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))  # (2)!\n[{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}]\n```\n\n```\nimport time\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph\nfrom langgraph.cache.memory import InMemoryCache\nfrom langgraph.types import CachePolicy\n\n\nclass State(TypedDict):\n    x: int\n    result: int\n\n\nbuilder = StateGraph(State)\n\n\ndef expensive_node(state: State) -> dict[str, int]:\n    # expensive computation\n    time.sleep(2)\n    return {\"result\": state[\"x\"] * 2}\n\n\nbuilder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\nbuilder.set_entry_point(\"expensive_node\")\nbuilder.set_finish_point(\"expensive_node\")\n\ngraph = builder.compile(cache=InMemoryCache())\n\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))  # (1)!\n[{'expensive_node': {'result': 10}}]\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))  # (2)!\n[{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}]\n```\nFirst run takes two seconds to run (due to mocked expensive computation).\n\nSecond run utilizes cache and returns quickly.\n\n\n\n## Edges¶\n\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:\n\nNormal Edges: Go directly from one node to the next.\n\nConditional Edges: Call a function to determine which node(s) to go to next.\n\nEntry Point: Which node to call first when user input arrives.\n\nConditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\n\nA node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.\n\n\n\n## Normal Edges¶\n\nIf you always want to go from node A to node B, you can use the add_edge method directly.\n\n\n```\ngraph.add_edge(\"node_a\", \"node_b\")\n```\n\n```\ngraph.add_edge(\"node_a\", \"node_b\")\n```\n\n\n## Conditional Edges¶\n\nIf you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a \"routing function\" to call after that node is executed:\n\n\n```\ngraph.add_conditional_edges(\"node_a\", routing_function)\n```\n\n```\ngraph.add_conditional_edges(\"node_a\", routing_function)\n```\nSimilar to nodes, the routing_function accepts the current state of the graph and returns a value.\n\n\n```\nrouting_function\n```\n\n```\nstate\n```\nBy default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\n\n\n```\nrouting_function\n```\nYou can optionally provide a dictionary that maps the routing_function's output to the name of the next node.\n\n\n```\nrouting_function\n```\n\n```\ngraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\n```\n\n```\ngraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\n```\nTip\n\nUse Command instead of conditional edges if you want to combine state updates and routing in a single function.\n\n\n```\nCommand\n```\n\n\n## Entry Point¶\n\nThe entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph.\n\n\n```\nadd_edge\n```\n\n```\nSTART\n```\nAPI Reference: START\n\n\n```\nfrom langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n```\n\n```\nfrom langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n```\n\n\n## Conditional Entry Point¶\n\nA conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this.\n\n\n```\nadd_conditional_edges\n```\n\n```\nSTART\n```\nAPI Reference: START\n\n\n```\nfrom langgraph.graph import START\n\ngraph.add_conditional_edges(START, routing_function)\n```\n\n```\nfrom langgraph.graph import START\n\ngraph.add_conditional_edges(START, routing_function)\n```\nYou can optionally provide a dictionary that maps the routing_function's output to the name of the next node.\n\n\n```\nrouting_function\n```\n\n```\ngraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})\n```\n\n```\ngraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})\n```\n\n\n## Send¶\n\n\n```\nSend\n```\nBy default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object).\n\n\n```\nNodes\n```\n\n```\nEdges\n```\n\n```\nState\n```\n\n```\nState\n```\n\n```\nNode\n```\nTo support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node.\n\n\n```\nSend\n```\n\n```\nSend\n```\n\n```\ndef continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n\ngraph.add_conditional_edges(\"node_a\", continue_to_jokes)\n```\n\n```\ndef continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n\ngraph.add_conditional_edges(\"node_a\", continue_to_jokes)\n```\n\n\n## Command¶\n\n\n```\nCommand\n```\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\n\n\n```\nCommand\n```\n\n```\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # state update\n        update={\"foo\": \"bar\"},\n        # control flow\n        goto=\"my_other_node\"\n    )\n```\n\n```\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # state update\n        update={\"foo\": \"bar\"},\n        # control flow\n        goto=\"my_other_node\"\n    )\n```\nWith Command you can also achieve dynamic control flow behavior (identical to conditional edges):\n\n\n```\nCommand\n```\n\n```\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    if state[\"foo\"] == \"bar\":\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\n```\n\n```\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    if state[\"foo\"] == \"bar\":\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\n```\nImportant\n\nWhen returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal[\"my_other_node\"]]. This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node.\n\n\n```\nCommand\n```\n\n```\nCommand[Literal[\"my_other_node\"]]\n```\n\n```\nmy_node\n```\n\n```\nmy_other_node\n```\nCheck out this how-to guide for an end-to-end example of how to use Command.\n\n\n```\nCommand\n```\n\n\n## When should I use Command instead of conditional edges?¶\n\nUse Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where it's important to route to a different agent and pass some information to that agent.\n\n\n```\nCommand\n```\nUse conditional edges to route between nodes conditionally without updating the state.\n\n\n\n## Navigating to a node in a parent graph¶\n\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\n\n\n```\ngraph=Command.PARENT\n```\n\n```\nCommand\n```\n\n```\ndef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\n    return Command(\n        update={\"foo\": \"bar\"},\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n        graph=Command.PARENT\n    )\n```\n\n```\ndef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\n    return Command(\n        update={\"foo\": \"bar\"},\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n        graph=Command.PARENT\n    )\n```\nNote\n\nSetting graph to Command.PARENT will navigate to the closest parent graph.\n\n\n```\ngraph\n```\n\n```\nCommand.PARENT\n```\nState updates with Command.PARENT\n\n\n```\nCommand.PARENT\n```\nWhen you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph state schemas, you must define a reducer for the key you're updating in the parent graph state. See this example.\n\nThis is particularly useful when implementing multi-agent handoffs.\n\nCheck out this guide for detail.\n\n\n\n## Using inside tools¶\n\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\n\nRefer to this guide for detail.\n\n\n\n## Human-in-the-loop¶\n\nCommand is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume=\"User input\"). Check out this conceptual guide for more information.\n\n\n```\nCommand\n```\n\n```\ninterrupt()\n```\n\n```\nCommand\n```\n\n```\nCommand(resume=\"User input\")\n```\n\n\n## Graph Migrations¶\n\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.\n\nFor threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\n\nFor threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.\n\nFor modifying state, we have full backwards and forwards compatibility for adding and removing keys\n\nState keys that are renamed lose their saved state in existing threads\n\nState keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.\n\n\n\n## Configuration¶\n\nWhen creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single \"cognitive architecture\" (the graph) but have multiple different instance of it.\n\nYou can optionally specify a config_schema when creating a graph.\n\n\n```\nconfig_schema\n```\n\n```\nclass ConfigSchema(TypedDict):\n    llm: str\n\ngraph = StateGraph(State, config_schema=ConfigSchema)\n```\n\n```\nclass ConfigSchema(TypedDict):\n    llm: str\n\ngraph = StateGraph(State, config_schema=ConfigSchema)\n```\nYou can then pass this configuration into the graph using the configurable config field.\n\n\n```\nconfigurable\n```\n\n```\nconfig = {\"configurable\": {\"llm\": \"anthropic\"}}\n\ngraph.invoke(inputs, config=config)\n```\n\n```\nconfig = {\"configurable\": {\"llm\": \"anthropic\"}}\n\ngraph.invoke(inputs, config=config)\n```\nYou can then access and use this configuration inside a node or conditional edge:\n\n\n```\ndef node_a(state, config):\n    llm_type = config.get(\"configurable\", {}).get(\"llm\", \"openai\")\n    llm = get_llm(llm_type)\n    ...\n```\n\n```\ndef node_a(state, config):\n    llm_type = config.get(\"configurable\", {}).get(\"llm\", \"openai\")\n    llm = get_llm(llm_type)\n    ...\n```\nSee this guide for a full breakdown on configuration.\n\n\n\n## Recursion Limit¶\n\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to .invoke/.stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:\n\n\n```\nGraphRecursionError\n```\n\n```\n.invoke\n```\n\n```\n.stream\n```\n\n```\nrecursion_limit\n```\n\n```\nconfig\n```\n\n```\nconfigurable\n```\n\n```\ngraph.invoke(inputs, config={\"recursion_limit\": 5, \"configurable\":{\"llm\": \"anthropic\"}})\n```\n\n```\ngraph.invoke(inputs, config={\"recursion_limit\": 5, \"configurable\":{\"llm\": \"anthropic\"}})\n```\nRead this how-to to learn more about how the recursion limit works.\n\n\n\n## Visualization¶\n\nIt's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.", "url": "https://langchain-ai.github.io/langgraph/concepts/low_level/", "path": "/langgraph/concepts/low_level/", "domain": "langchain-ai.github.io", "source": "LangGraph 공식문서", "content_length": 45880, "crawled_at": "2025-07-18 03:24:38"}
{"title": "Persistence¶", "content": "Get started\n\nGuides\n    \n  \n\n\n\n\n\n\n\n\n            Guides\n          \n\n\n\n\n\n    Agent development\n    \n  \n\n\n\n\n\n            Agent development\n          \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n\n    Run an agent\n    \n  \n\n\n\n\n\n\n\n\n\n    LangGraph APIs\n    \n  \n\n\n\n\n\n            LangGraph APIs\n          \n\n\n\n\n    Graph API\n    \n  \n\n\n\n\n\n\n    Functional API\n    \n  \n\n\n\n\n\n\n    Runtime\n    \n  \n\n\n\n\n\n\n\n\n\n    Core capabilities\n    \n  \n\n\n\n\n\n            Core capabilities\n          \n\n\n\n\n    Streaming\n    \n  \n\n\n\n\n\n\n\n    Persistence\n    \n  \n\n\n\n\n\n            Persistence\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Threads\n    \n\n\n\n\n\n      Checkpoints\n    \n\n\n\n\n\n\n      Get state\n    \n\n\n\n\n\n      Get state history\n    \n\n\n\n\n\n      Replay\n    \n\n\n\n\n\n      Update state\n    \n\n\n\n\n\n\n      config\n    \n\n\n\n\n\n      values\n    \n\n\n\n\n\n      as_node\n    \n\n\n\n\n\n\n\n\n\n\n\n      Memory Store\n    \n\n\n\n\n\n\n      Basic Usage\n    \n\n\n\n\n\n      Semantic Search\n    \n\n\n\n\n\n      Using in LangGraph\n    \n\n\n\n\n\n\n\n\n      Checkpointer libraries\n    \n\n\n\n\n\n\n      Checkpointer interface\n    \n\n\n\n\n\n      Serializer\n    \n\n\n\n\n\n\n      Serialization with pickle\n    \n\n\n\n\n\n      Encryption\n    \n\n\n\n\n\n\n\n\n\n\n\n      Capabilities\n    \n\n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n      Memory\n    \n\n\n\n\n\n      Time Travel\n    \n\n\n\n\n\n      Fault-tolerance\n    \n\n\n\n\n\n\n      Pending writes\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Durable execution\n    \n  \n\n\n\n\n\n\n    Memory\n    \n  \n\n\n\n\n\n\n    Context\n    \n  \n\n\n\n\n\n\n    Models\n    \n  \n\n\n\n\n\n\n    Tools\n    \n  \n\n\n\n\n\n\n    Human-in-the-loop\n    \n  \n\n\n\n\n\n\n    Time travel\n    \n  \n\n\n\n\n\n\n    Subgraphs\n    \n  \n\n\n\n\n\n\n    Multi-agent\n    \n  \n\n\n\n\n\n\n    MCP\n    \n  \n\n\n\n\n\n\n    Tracing\n    \n  \n\n\n\n\n\n\n\n\n\n\n    Platform-only capabilities\n    \n  \n\n\n\n\n\n            Platform-only capabilities\n          \n\n\n\n\n    LangGraph Platform\n    \n  \n\n\n\n\n\n\n    Authentication & access control\n    \n  \n\n\n\n\n\n\n    Assistants\n    \n  \n\n\n\n\n\n\n    Double-texting\n    \n  \n\n\n\n\n\n\n    Webhooks\n    \n  \n\n\n\n\n\n\n    Cron jobs\n    \n  \n\n\n\n\n\n\n    Server customization\n    \n  \n\n\n\n\n\n\n    Data management\n    \n  \n\n\n\n\n\n\n    Deployment\n\nAgent development\n    \n  \n\n\n\n\n\n            Agent development\n          \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n\n    Run an agent\n\nOverview\n\nRun an agent\n\nLangGraph APIs\n    \n  \n\n\n\n\n\n            LangGraph APIs\n          \n\n\n\n\n    Graph API\n    \n  \n\n\n\n\n\n\n    Functional API\n    \n  \n\n\n\n\n\n\n    Runtime\n\nGraph API\n\nFunctional API\n\nRuntime\n\nCore capabilities\n    \n  \n\n\n\n\n\n            Core capabilities\n          \n\n\n\n\n    Streaming\n    \n  \n\n\n\n\n\n\n\n    Persistence\n    \n  \n\n\n\n\n\n            Persistence\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Threads\n    \n\n\n\n\n\n      Checkpoints\n    \n\n\n\n\n\n\n      Get state\n    \n\n\n\n\n\n      Get state history\n    \n\n\n\n\n\n      Replay\n    \n\n\n\n\n\n      Update state\n    \n\n\n\n\n\n\n      config\n    \n\n\n\n\n\n      values\n    \n\n\n\n\n\n      as_node\n    \n\n\n\n\n\n\n\n\n\n\n\n      Memory Store\n    \n\n\n\n\n\n\n      Basic Usage\n    \n\n\n\n\n\n      Semantic Search\n    \n\n\n\n\n\n      Using in LangGraph\n    \n\n\n\n\n\n\n\n\n      Checkpointer libraries\n    \n\n\n\n\n\n\n      Checkpointer interface\n    \n\n\n\n\n\n      Serializer\n    \n\n\n\n\n\n\n      Serialization with pickle\n    \n\n\n\n\n\n      Encryption\n    \n\n\n\n\n\n\n\n\n\n\n\n      Capabilities\n    \n\n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n      Memory\n    \n\n\n\n\n\n      Time Travel\n    \n\n\n\n\n\n      Fault-tolerance\n    \n\n\n\n\n\n\n      Pending writes\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Durable execution\n    \n  \n\n\n\n\n\n\n    Memory\n    \n  \n\n\n\n\n\n\n    Context\n    \n  \n\n\n\n\n\n\n    Models\n    \n  \n\n\n\n\n\n\n    Tools\n    \n  \n\n\n\n\n\n\n    Human-in-the-loop\n    \n  \n\n\n\n\n\n\n    Time travel\n    \n  \n\n\n\n\n\n\n    Subgraphs\n    \n  \n\n\n\n\n\n\n    Multi-agent\n    \n  \n\n\n\n\n\n\n    MCP\n    \n  \n\n\n\n\n\n\n    Tracing\n\nStreaming\n\nPersistence\n    \n  \n\n\n\n\n\n            Persistence\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Threads\n    \n\n\n\n\n\n      Checkpoints\n    \n\n\n\n\n\n\n      Get state\n    \n\n\n\n\n\n      Get state history\n    \n\n\n\n\n\n      Replay\n    \n\n\n\n\n\n      Update state\n    \n\n\n\n\n\n\n      config\n    \n\n\n\n\n\n      values\n    \n\n\n\n\n\n      as_node\n    \n\n\n\n\n\n\n\n\n\n\n\n      Memory Store\n    \n\n\n\n\n\n\n      Basic Usage\n    \n\n\n\n\n\n      Semantic Search\n    \n\n\n\n\n\n      Using in LangGraph\n    \n\n\n\n\n\n\n\n\n      Checkpointer libraries\n    \n\n\n\n\n\n\n      Checkpointer interface\n    \n\n\n\n\n\n      Serializer\n    \n\n\n\n\n\n\n      Serialization with pickle\n    \n\n\n\n\n\n      Encryption\n    \n\n\n\n\n\n\n\n\n\n\n\n      Capabilities\n    \n\n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n      Memory\n    \n\n\n\n\n\n      Time Travel\n    \n\n\n\n\n\n      Fault-tolerance\n    \n\n\n\n\n\n\n      Pending writes\n\nOverview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Threads\n    \n\n\n\n\n\n      Checkpoints\n    \n\n\n\n\n\n\n      Get state\n    \n\n\n\n\n\n      Get state history\n    \n\n\n\n\n\n      Replay\n    \n\n\n\n\n\n      Update state\n    \n\n\n\n\n\n\n      config\n    \n\n\n\n\n\n      values\n    \n\n\n\n\n\n      as_node\n    \n\n\n\n\n\n\n\n\n\n\n\n      Memory Store\n    \n\n\n\n\n\n\n      Basic Usage\n    \n\n\n\n\n\n      Semantic Search\n    \n\n\n\n\n\n      Using in LangGraph\n    \n\n\n\n\n\n\n\n\n      Checkpointer libraries\n    \n\n\n\n\n\n\n      Checkpointer interface\n    \n\n\n\n\n\n      Serializer\n    \n\n\n\n\n\n\n      Serialization with pickle\n    \n\n\n\n\n\n      Encryption\n    \n\n\n\n\n\n\n\n\n\n\n\n      Capabilities\n    \n\n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n      Memory\n    \n\n\n\n\n\n      Time Travel\n    \n\n\n\n\n\n      Fault-tolerance\n    \n\n\n\n\n\n\n      Pending writes\n\nThreads\n\nCheckpoints\n    \n\n\n\n\n\n\n      Get state\n    \n\n\n\n\n\n      Get state history\n    \n\n\n\n\n\n      Replay\n    \n\n\n\n\n\n      Update state\n    \n\n\n\n\n\n\n      config\n    \n\n\n\n\n\n      values\n    \n\n\n\n\n\n      as_node\n\nGet state\n\nGet state history\n\nReplay\n\nUpdate state\n    \n\n\n\n\n\n\n      config\n    \n\n\n\n\n\n      values\n    \n\n\n\n\n\n      as_node\n\nconfig\n\nvalues\n\nas_node\n\nMemory Store\n    \n\n\n\n\n\n\n      Basic Usage\n    \n\n\n\n\n\n      Semantic Search\n    \n\n\n\n\n\n      Using in LangGraph\n\nBasic Usage\n\nSemantic Search\n\nUsing in LangGraph\n\nCheckpointer libraries\n    \n\n\n\n\n\n\n      Checkpointer interface\n    \n\n\n\n\n\n      Serializer\n    \n\n\n\n\n\n\n      Serialization with pickle\n    \n\n\n\n\n\n      Encryption\n\nCheckpointer interface\n\nSerializer\n    \n\n\n\n\n\n\n      Serialization with pickle\n    \n\n\n\n\n\n      Encryption\n\nSerialization with pickle\n\nEncryption\n\nCapabilities\n    \n\n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n      Memory\n    \n\n\n\n\n\n      Time Travel\n    \n\n\n\n\n\n      Fault-tolerance\n    \n\n\n\n\n\n\n      Pending writes\n\nHuman-in-the-loop\n\nMemory\n\nTime Travel\n\nFault-tolerance\n    \n\n\n\n\n\n\n      Pending writes\n\nPending writes\n\nDurable execution\n\nMemory\n\nContext\n\nModels\n\nTools\n\nHuman-in-the-loop\n\nTime travel\n\nSubgraphs\n\nMulti-agent\n\nMCP\n\nTracing\n\nPlatform-only capabilities\n    \n  \n\n\n\n\n\n            Platform-only capabilities\n          \n\n\n\n\n    LangGraph Platform\n    \n  \n\n\n\n\n\n\n    Authentication & access control\n    \n  \n\n\n\n\n\n\n    Assistants\n    \n  \n\n\n\n\n\n\n    Double-texting\n    \n  \n\n\n\n\n\n\n    Webhooks\n    \n  \n\n\n\n\n\n\n    Cron jobs\n    \n  \n\n\n\n\n\n\n    Server customization\n    \n  \n\n\n\n\n\n\n    Data management\n    \n  \n\n\n\n\n\n\n    Deployment\n\nLangGraph Platform\n\nAuthentication & access control\n\nAssistants\n\nDouble-texting\n\nWebhooks\n\nCron jobs\n\nServer customization\n\nData management\n\nDeployment\n\nReference\n\nExamples\n\nAdditional resources\n\nThreads\n\nCheckpoints\n    \n\n\n\n\n\n\n      Get state\n    \n\n\n\n\n\n      Get state history\n    \n\n\n\n\n\n      Replay\n    \n\n\n\n\n\n      Update state\n    \n\n\n\n\n\n\n      config\n    \n\n\n\n\n\n      values\n    \n\n\n\n\n\n      as_node\n\nGet state\n\nGet state history\n\nReplay\n\nUpdate state\n    \n\n\n\n\n\n\n      config\n    \n\n\n\n\n\n      values\n    \n\n\n\n\n\n      as_node\n\nconfig\n\nvalues\n\nas_node\n\nMemory Store\n    \n\n\n\n\n\n\n      Basic Usage\n    \n\n\n\n\n\n      Semantic Search\n    \n\n\n\n\n\n      Using in LangGraph\n\nBasic Usage\n\nSemantic Search\n\nUsing in LangGraph\n\nCheckpointer libraries\n    \n\n\n\n\n\n\n      Checkpointer interface\n    \n\n\n\n\n\n      Serializer\n    \n\n\n\n\n\n\n      Serialization with pickle\n    \n\n\n\n\n\n      Encryption\n\nCheckpointer interface\n\nSerializer\n    \n\n\n\n\n\n\n      Serialization with pickle\n    \n\n\n\n\n\n      Encryption\n\nSerialization with pickle\n\nEncryption\n\nCapabilities\n    \n\n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n      Memory\n    \n\n\n\n\n\n      Time Travel\n    \n\n\n\n\n\n      Fault-tolerance\n    \n\n\n\n\n\n\n      Pending writes\n\nHuman-in-the-loop\n\nMemory\n\nTime Travel\n\nFault-tolerance\n    \n\n\n\n\n\n\n      Pending writes\n\nPending writes\n\n\n\n## Persistence¶\n\nLangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread, which can be accessed after graph execution. Because threads allow access to graph's state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we'll discuss each of these concepts in more detail.\n\n\n```\ncheckpoint\n```\n\n```\nthread\n```\n\n```\nthreads\n```\nLangGraph API handles checkpointing automatically\n\nWhen using the LangGraph API, you don't need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes.\n\n\n\n## Threads¶\n\nA thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread.\n\nWhen invoking graph with a checkpointer, you must specify a thread_id as part of the configurable portion of the config:\n\n\n```\nthread_id\n```\n\n```\nconfigurable\n```\n\n```\n{\"configurable\": {\"thread_id\": \"1\"}}\n```\n\n```\n{\"configurable\": {\"thread_id\": \"1\"}}\n```\nA thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangGraph Platform API provides several endpoints for creating and managing threads and thread state. See the API reference for more details.\n\n\n\n## Checkpoints¶\n\nThe state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by StateSnapshot object with the following key properties:\n\n\n```\nStateSnapshot\n```\nconfig: Config associated with this checkpoint.\n\n\n```\nconfig\n```\nmetadata: Metadata associated with this checkpoint.\n\n\n```\nmetadata\n```\nvalues: Values of the state channels at this point in time.\n\n\n```\nvalues\n```\nnext A tuple of the node names to execute next in the graph.\n\n\n```\nnext\n```\ntasks: A tuple of PregelTask objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts.\n\n\n```\ntasks\n```\n\n```\nPregelTask\n```\nCheckpoints are persisted and can be used to restore the state of a thread at a later time.\n\nLet's see what checkpoints are saved when a simple graph is invoked as follows:\n\nAPI Reference: StateGraph | START | END | InMemorySaver\n\n\n```\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: str\n    bar: Annotated[list[str], add]\n\ndef node_a(state: State):\n    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n\ndef node_b(state: State):\n    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(node_a)\nworkflow.add_node(node_b)\nworkflow.add_edge(START, \"node_a\")\nworkflow.add_edge(\"node_a\", \"node_b\")\nworkflow.add_edge(\"node_b\", END)\n\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"foo\": \"\"}, config)\n```\n\n```\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: str\n    bar: Annotated[list[str], add]\n\ndef node_a(state: State):\n    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n\ndef node_b(state: State):\n    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(node_a)\nworkflow.add_node(node_b)\nworkflow.add_edge(START, \"node_a\")\nworkflow.add_edge(\"node_a\", \"node_b\")\nworkflow.add_edge(\"node_b\", END)\n\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"foo\": \"\"}, config)\n```\nAfter we run the graph, we expect to see exactly 4 checkpoints:\n\nempty checkpoint with START as the next node to be executed\n\n\n```\nSTART\n```\ncheckpoint with the user input {'foo': '', 'bar': []} and node_a as the next node to be executed\n\n\n```\n{'foo': '', 'bar': []}\n```\n\n```\nnode_a\n```\ncheckpoint with the outputs of node_a {'foo': 'a', 'bar': ['a']} and node_b as the next node to be executed\n\n\n```\nnode_a\n```\n\n```\n{'foo': 'a', 'bar': ['a']}\n```\n\n```\nnode_b\n```\ncheckpoint with the outputs of node_b {'foo': 'b', 'bar': ['a', 'b']} and no next nodes to be executed\n\n\n```\nnode_b\n```\n\n```\n{'foo': 'b', 'bar': ['a', 'b']}\n```\nNote that the bar channel values contain outputs from both nodes as we have a reducer for bar channel.\n\n\n```\nbar\n```\n\n```\nbar\n```\n\n\n## Get state¶\n\nWhen interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by calling graph.get_state(config). This will return a StateSnapshot object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.\n\n\n```\ngraph.get_state(config)\n```\n\n```\nStateSnapshot\n```\n\n```\n# get the latest state snapshot\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.get_state(config)\n\n# get a state snapshot for a specific checkpoint_id\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1ef663ba-28fe-6528-8002-5a559208592c\"}}\ngraph.get_state(config)\n```\n\n```\n# get the latest state snapshot\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.get_state(config)\n\n# get a state snapshot for a specific checkpoint_id\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1ef663ba-28fe-6528-8002-5a559208592c\"}}\ngraph.get_state(config)\n```\nIn our example, the output of get_state will look like this:\n\n\n```\nget_state\n```\n\n```\nStateSnapshot(\n    values={'foo': 'b', 'bar': ['a', 'b']},\n    next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n    created_at='2024-08-29T19:19:38.821749+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()\n)\n```\n\n```\nStateSnapshot(\n    values={'foo': 'b', 'bar': ['a', 'b']},\n    next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n    created_at='2024-08-29T19:19:38.821749+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()\n)\n```\n\n\n## Get state history¶\n\nYou can get the full history of the graph execution for a given thread by calling graph.get_state_history(config). This will return a list of StateSnapshot objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / StateSnapshot being the first in the list.\n\n\n```\ngraph.get_state_history(config)\n```\n\n```\nStateSnapshot\n```\n\n```\nStateSnapshot\n```\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\nlist(graph.get_state_history(config))\n```\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\nlist(graph.get_state_history(config))\n```\nIn our example, the output of get_state_history will look like this:\n\n\n```\nget_state_history\n```\n\n```\n[\n    StateSnapshot(\n        values={'foo': 'b', 'bar': ['a', 'b']},\n        next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n        metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n        created_at='2024-08-29T19:19:38.821749+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\n        tasks=(),\n    ),\n    StateSnapshot(\n        values={'foo': 'a', 'bar': ['a']}, next=('node_b',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\n        metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},\n        created_at='2024-08-29T19:19:38.819946+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\n        tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),\n    ),\n    StateSnapshot(\n        values={'foo': '', 'bar': []},\n        next=('node_a',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\n        metadata={'source': 'loop', 'writes': None, 'step': 0},\n        created_at='2024-08-29T19:19:38.817813+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\n        tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),\n    ),\n    StateSnapshot(\n        values={'bar': []},\n        next=('__start__',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\n        metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},\n        created_at='2024-08-29T19:19:38.816205+00:00',\n        parent_config=None,\n        tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),\n    )\n]\n```\n\n```\n[\n    StateSnapshot(\n        values={'foo': 'b', 'bar': ['a', 'b']},\n        next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n        metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n        created_at='2024-08-29T19:19:38.821749+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\n        tasks=(),\n    ),\n    StateSnapshot(\n        values={'foo': 'a', 'bar': ['a']}, next=('node_b',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\n        metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},\n        created_at='2024-08-29T19:19:38.819946+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\n        tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),\n    ),\n    StateSnapshot(\n        values={'foo': '', 'bar': []},\n        next=('node_a',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\n        metadata={'source': 'loop', 'writes': None, 'step': 0},\n        created_at='2024-08-29T19:19:38.817813+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\n        tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),\n    ),\n    StateSnapshot(\n        values={'bar': []},\n        next=('__start__',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\n        metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},\n        created_at='2024-08-29T19:19:38.816205+00:00',\n        parent_config=None,\n        tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),\n    )\n]\n```\n\n\n## Replay¶\n\nIt's also possible to play-back a prior graph execution. If we invoke a graph with a thread_id and a checkpoint_id, then we will re-play the previously executed steps before a checkpoint that corresponds to the checkpoint_id, and only execute the steps after the checkpoint.\n\n\n```\ninvoke\n```\n\n```\nthread_id\n```\n\n```\ncheckpoint_id\n```\n\n```\ncheckpoint_id\n```\nthread_id is the ID of a thread.\n\n\n```\nthread_id\n```\ncheckpoint_id is an identifier that refers to a specific checkpoint within a thread.\n\n\n```\ncheckpoint_id\n```\nYou must pass these when invoking the graph as part of the configurable portion of the config:\n\n\n```\nconfigurable\n```\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"}}\ngraph.invoke(None, config=config)\n```\n\n```\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"}}\ngraph.invoke(None, config=config)\n```\nImportantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply re-plays that particular step in the graph and does not re-execute the step, but only for the steps before the provided checkpoint_id. All of the steps after checkpoint_id will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying.\n\n\n```\ncheckpoint_id\n```\n\n```\ncheckpoint_id\n```\n\n\n## Update state¶\n\nIn addition to re-playing the graph from specific checkpoints, we can also edit the graph state. We do this using graph.update_state(). This method accepts three different arguments:\n\n\n```\ncheckpoints\n```\n\n```\ngraph.update_state()\n```\n\n\n## config¶\n\n\n```\nconfig\n```\nThe config should contain thread_id specifying which thread to update. When only the thread_id is passed, we update (or fork) the current state. Optionally, if we include checkpoint_id field, then we fork that selected checkpoint.\n\n\n```\nthread_id\n```\n\n```\nthread_id\n```\n\n```\ncheckpoint_id\n```\n\n\n## values¶\n\n\n```\nvalues\n```\nThese are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that update_state does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let's walk through an example.\n\n\n```\nupdate_state\n```\nLet's assume you have defined the state of your graph with the following schema (see full example above):\n\n\n```\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n```\n\n```\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n```\nLet's now assume the current state of the graph is\n\n\n```\n{\"foo\": 1, \"bar\": [\"a\"]}\n```\n\n```\n{\"foo\": 1, \"bar\": [\"a\"]}\n```\nIf you update the state as below:\n\n\n```\ngraph.update_state(config, {\"foo\": 2, \"bar\": [\"b\"]})\n```\n\n```\ngraph.update_state(config, {\"foo\": 2, \"bar\": [\"b\"]})\n```\nThen the new state of the graph will be:\n\n\n```\n{\"foo\": 2, \"bar\": [\"a\", \"b\"]}\n```\n\n```\n{\"foo\": 2, \"bar\": [\"a\", \"b\"]}\n```\nThe foo key (channel) is completely changed (because there is no reducer specified for that channel, so update_state overwrites it). However, there is a reducer specified for the bar key, and so it appends \"b\" to the state of bar.\n\n\n```\nfoo\n```\n\n```\nupdate_state\n```\n\n```\nbar\n```\n\n```\n\"b\"\n```\n\n```\nbar\n```\n\n\n## as_node¶\n\n\n```\nas_node\n```\nThe final thing you can optionally specify when calling update_state is as_node. If you provided it, the update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state.\n\n\n```\nupdate_state\n```\n\n```\nas_node\n```\n\n```\nas_node\n```\n\n```\nas_node\n```\n\n\n## Memory Store¶\n\nA state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence.\n\nBut, what if we want to retain some information across threads? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user!\n\nWith checkpointers alone, we cannot share information across threads. This motivates the need for the Store interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new in_memory_store variable.\n\n\n```\nStore\n```\n\n```\nInMemoryStore\n```\n\n```\nin_memory_store\n```\nLangGraph API handles stores automatically\n\nWhen using the LangGraph API, you don't need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.\n\n\n\n## Basic Usage¶\n\nFirst, let's showcase this in isolation without using LangGraph.\n\n\n```\nfrom langgraph.store.memory import InMemoryStore\nin_memory_store = InMemoryStore()\n```\n\n```\nfrom langgraph.store.memory import InMemoryStore\nin_memory_store = InMemoryStore()\n```\nMemories are namespaced by a tuple, which in this specific example will be (<user_id>, \"memories\"). The namespace can be any length and represent anything, does not have to be user specific.\n\n\n```\ntuple\n```\n\n```\n(<user_id>, \"memories\")\n```\n\n```\nuser_id = \"1\"\nnamespace_for_memory = (user_id, \"memories\")\n```\n\n```\nuser_id = \"1\"\nnamespace_for_memory = (user_id, \"memories\")\n```\nWe use the store.put method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (memory_id) and the value (a dictionary) is the memory itself.\n\n\n```\nstore.put\n```\n\n```\nmemory_id\n```\n\n```\nmemory_id = str(uuid.uuid4())\nmemory = {\"food_preference\" : \"I like pizza\"}\nin_memory_store.put(namespace_for_memory, memory_id, memory)\n```\n\n```\nmemory_id = str(uuid.uuid4())\nmemory = {\"food_preference\" : \"I like pizza\"}\nin_memory_store.put(namespace_for_memory, memory_id, memory)\n```\nWe can read out memories in our namespace using the store.search method, which will return all memories for a given user as a list. The most recent memory is the last in the list.\n\n\n```\nstore.search\n```\n\n```\nmemories = in_memory_store.search(namespace_for_memory)\nmemories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n```\n\n```\nmemories = in_memory_store.search(namespace_for_memory)\nmemories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n```\nEach memory type is a Python class (Item) with certain attributes. We can access it as a dictionary by converting via .dict as above.\nThe attributes it has are:\n\n\n```\nItem\n```\n\n```\n.dict\n```\nvalue: The value (itself a dictionary) of this memory\n\n\n```\nvalue\n```\nkey: A unique key for this memory in this namespace\n\n\n```\nkey\n```\nnamespace: A list of strings, the namespace of this memory type\n\n\n```\nnamespace\n```\ncreated_at: Timestamp for when this memory was created\n\n\n```\ncreated_at\n```\nupdated_at: Timestamp for when this memory was updated\n\n\n```\nupdated_at\n```\n\n\n## Semantic Search¶\n\nBeyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:\n\nAPI Reference: init_embeddings\n\n\n```\nfrom langchain.embeddings import init_embeddings\n\nstore = InMemoryStore(\n    index={\n        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\n        \"dims\": 1536,                              # Embedding dimensions\n        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\n    }\n)\n```\n\n```\nfrom langchain.embeddings import init_embeddings\n\nstore = InMemoryStore(\n    index={\n        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\n        \"dims\": 1536,                              # Embedding dimensions\n        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\n    }\n)\n```\nNow when searching, you can use natural language queries to find relevant memories:\n\n\n```\n# Find memories about food preferences\n# (This can be done after putting memories into the store)\nmemories = store.search(\n    namespace_for_memory,\n    query=\"What does the user like to eat?\",\n    limit=3  # Return top 3 matches\n)\n```\n\n```\n# Find memories about food preferences\n# (This can be done after putting memories into the store)\nmemories = store.search(\n    namespace_for_memory,\n    query=\"What does the user like to eat?\",\n    limit=3  # Return top 3 matches\n)\n```\nYou can control which parts of your memories get embedded by configuring the fields parameter or by specifying the index parameter when storing memories:\n\n\n```\nfields\n```\n\n```\nindex\n```\n\n```\n# Store with specific fields to embed\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\n        \"food_preference\": \"I love Italian cuisine\",\n        \"context\": \"Discussing dinner plans\"\n    },\n    index=[\"food_preference\"]  # Only embed \"food_preferences\" field\n)\n\n# Store without embedding (still retrievable, but not searchable)\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\"system_info\": \"Last updated: 2024-01-01\"},\n    index=False\n)\n```\n\n```\n# Store with specific fields to embed\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\n        \"food_preference\": \"I love Italian cuisine\",\n        \"context\": \"Discussing dinner plans\"\n    },\n    index=[\"food_preference\"]  # Only embed \"food_preferences\" field\n)\n\n# Store without embedding (still retrievable, but not searchable)\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\"system_info\": \"Last updated: 2024-01-01\"},\n    index=False\n)\n```\n\n\n## Using in LangGraph¶\n\nWith this all in place, we use the in_memory_store in LangGraph. The in_memory_store works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the in_memory_store allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the in_memory_store as follows.\n\n\n```\nin_memory_store\n```\n\n```\nin_memory_store\n```\n\n```\nin_memory_store\n```\n\n```\nin_memory_store\n```\nAPI Reference: InMemorySaver\n\n\n```\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# We need this because we want to enable threads (conversations)\ncheckpointer = InMemorySaver()\n\n# ... Define the graph ...\n\n# Compile the graph with the checkpointer and store\ngraph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n```\n\n```\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# We need this because we want to enable threads (conversations)\ncheckpointer = InMemorySaver()\n\n# ... Define the graph ...\n\n# Compile the graph with the checkpointer and store\ngraph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n```\nWe invoke the graph with a thread_id, as before, and also with a user_id, which we'll use to namespace our memories to this particular user as we showed above.\n\n\n```\nthread_id\n```\n\n```\nuser_id\n```\n\n```\n# Invoke the graph\nuser_id = \"1\"\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\n\n# First let's just say hi to the AI\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n```\n\n```\n# Invoke the graph\nuser_id = \"1\"\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\n\n# First let's just say hi to the AI\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n```\nWe can access the in_memory_store and the user_id in any node by passing store: BaseStore and config: RunnableConfig as node arguments. Here's how we might use semantic search in a node to find relevant memories:\n\n\n```\nin_memory_store\n```\n\n```\nuser_id\n```\n\n```\nstore: BaseStore\n```\n\n```\nconfig: RunnableConfig\n```\n\n```\ndef update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # ... Analyze conversation and create a new memory\n\n    # Create a new memory ID\n    memory_id = str(uuid.uuid4())\n\n    # We create a new memory\n    store.put(namespace, memory_id, {\"memory\": memory})\n```\n\n```\ndef update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # ... Analyze conversation and create a new memory\n\n    # Create a new memory ID\n    memory_id = str(uuid.uuid4())\n\n    # We create a new memory\n    store.put(namespace, memory_id, {\"memory\": memory})\n```\nAs we showed above, we can also access the store in any node and use the store.search method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.\n\n\n```\nstore.search\n```\n\n```\nmemories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n```\n\n```\nmemories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n```\nWe can access the memories and use them in our model call.\n\n\n```\ndef call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # Search based on the most recent message\n    memories = store.search(\n        namespace,\n        query=state[\"messages\"][-1].content,\n        limit=3\n    )\n    info = \"\\n\".join([d.value[\"memory\"] for d in memories])\n\n    # ... Use memories in the model call\n```\n\n```\ndef call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # Search based on the most recent message\n    memories = store.search(\n        namespace,\n        query=state[\"messages\"][-1].content,\n        limit=3\n    )\n    info = \"\\n\".join([d.value[\"memory\"] for d in memories])\n\n    # ... Use memories in the model call\n```\nIf we create a new thread, we can still access the same memories so long as the user_id is the same.\n\n\n```\nuser_id\n```\n\n```\n# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n\n# Let's say hi again\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi, tell me about my memories\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n```\n\n```\n# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n\n# Let's say hi again\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi, tell me about my memories\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n```\nWhen we use the LangGraph Platform, either locally (e.g., in LangGraph Studio) or with LangGraph Platform, the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you do need to configure the indexing settings in your langgraph.json file. For example:\n\n\n```\nlanggraph.json\n```\n\n```\n{\n    ...\n    \"store\": {\n        \"index\": {\n            \"embed\": \"openai:text-embeddings-3-small\",\n            \"dims\": 1536,\n            \"fields\": [\"$\"]\n        }\n    }\n}\n```\n\n```\n{\n    ...\n    \"store\": {\n        \"index\": {\n            \"embed\": \"openai:text-embeddings-3-small\",\n            \"dims\": 1536,\n            \"fields\": [\"$\"]\n        }\n    }\n}\n```\nSee the deployment guide for more details and configuration options.\n\n\n\n## Checkpointer libraries¶\n\nUnder the hood, checkpointing is powered by checkpointer objects that conform to BaseCheckpointSaver interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:\n\nlanggraph-checkpoint: The base interface for checkpointer savers (BaseCheckpointSaver) and serialization/deserialization interface (SerializerProtocol). Includes in-memory checkpointer implementation (InMemorySaver) for experimentation. LangGraph comes with langgraph-checkpoint included.\n\n\n```\nlanggraph-checkpoint\n```\n\n```\nlanggraph-checkpoint\n```\nlanggraph-checkpoint-sqlite: An implementation of LangGraph checkpointer that uses SQLite database (SqliteSaver / AsyncSqliteSaver). Ideal for experimentation and local workflows. Needs to be installed separately.\n\n\n```\nlanggraph-checkpoint-sqlite\n```\nlanggraph-checkpoint-postgres: An advanced checkpointer that uses Postgres database (PostgresSaver / AsyncPostgresSaver), used in LangGraph Platform. Ideal for using in production. Needs to be installed separately.\n\n\n```\nlanggraph-checkpoint-postgres\n```\n\n\n## Checkpointer interface¶\n\nEach checkpointer conforms to BaseCheckpointSaver interface and implements the following methods:\n\n.put - Store a checkpoint with its configuration and metadata.\n\n\n```\n.put\n```\n.put_writes - Store intermediate writes linked to a checkpoint (i.e. pending writes).\n\n\n```\n.put_writes\n```\n.get_tuple - Fetch a checkpoint tuple using for a given configuration (thread_id and checkpoint_id). This is used to populate StateSnapshot in graph.get_state().\n\n\n```\n.get_tuple\n```\n\n```\nthread_id\n```\n\n```\ncheckpoint_id\n```\n\n```\nStateSnapshot\n```\n\n```\ngraph.get_state()\n```\n.list - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in graph.get_state_history()\n\n\n```\n.list\n```\n\n```\ngraph.get_state_history()\n```\nIf the checkpointer is used with asynchronous graph execution (i.e. executing the graph via .ainvoke, .astream, .abatch), asynchronous versions of the above methods will be used (.aput, .aput_writes, .aget_tuple, .alist).\n\n\n```\n.ainvoke\n```\n\n```\n.astream\n```\n\n```\n.abatch\n```\n\n```\n.aput\n```\n\n```\n.aput_writes\n```\n\n```\n.aget_tuple\n```\n\n```\n.alist\n```\nNote\n\nFor running your graph asynchronously, you can use InMemorySaver, or async versions of Sqlite/Postgres checkpointers -- AsyncSqliteSaver / AsyncPostgresSaver checkpointers.\n\n\n```\nInMemorySaver\n```\n\n```\nAsyncSqliteSaver\n```\n\n```\nAsyncPostgresSaver\n```\n\n\n## Serializer¶\n\nWhen checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.\nlanggraph_checkpoint defines protocol for implementing serializers provides a default implementation (JsonPlusSerializer) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.\n\n\n```\nlanggraph_checkpoint\n```\n\n\n## Serialization with pickle¶\n\n\n```\npickle\n```\nThe default serializer, JsonPlusSerializer, uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.\n\n\n```\nJsonPlusSerializer\n```\nIf you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes),\nyou can use the pickle_fallback argument of the JsonPlusSerializer:\n\n\n```\npickle_fallback\n```\n\n```\nJsonPlusSerializer\n```\nAPI Reference: MemorySaver | JsonPlusSerializer\n\n\n```\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\n\n# ... Define the graph ...\ngraph.compile(\n    checkpointer=MemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))\n)\n```\n\n```\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\n\n# ... Define the graph ...\ngraph.compile(\n    checkpointer=MemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))\n)\n```\n\n\n## Encryption¶\n\nCheckpointers can optionally encrypt all persisted state. To enable this, pass an instance of EncryptedSerializer to the serde argument of any BaseCheckpointSaver implementation. The easiest way to create an encrypted serializer is via from_pycryptodome_aes, which reads the AES key from the LANGGRAPH_AES_KEY environment variable (or accepts a key argument):\n\n\n```\nEncryptedSerializer\n```\n\n```\nserde\n```\n\n```\nBaseCheckpointSaver\n```\n\n```\nfrom_pycryptodome_aes\n```\n\n```\nLANGGRAPH_AES_KEY\n```\n\n```\nkey\n```\nAPI Reference: SqliteSaver\n\n\n```\nimport sqlite3\n\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.sqlite import SqliteSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY\ncheckpointer = SqliteSaver(sqlite3.connect(\"checkpoint.db\"), serde=serde)\n```\n\n```\nimport sqlite3\n\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.sqlite import SqliteSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY\ncheckpointer = SqliteSaver(sqlite3.connect(\"checkpoint.db\"), serde=serde)\n```\nAPI Reference: PostgresSaver\n\n\n```\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()\ncheckpointer = PostgresSaver.from_conn_string(\"postgresql://...\", serde=serde)\ncheckpointer.setup()\n```\n\n```\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()\ncheckpointer = PostgresSaver.from_conn_string(\"postgresql://...\", serde=serde)\ncheckpointer.setup()\n```\nWhen running on LangGraph Platform, encryption is automatically enabled whenever LANGGRAPH_AES_KEY is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing CipherProtocol and supplying it to EncryptedSerializer.\n\n\n```\nLANGGRAPH_AES_KEY\n```\n\n```\nCipherProtocol\n```\n\n```\nEncryptedSerializer\n```\n\n\n## Capabilities¶\n\n\n\n## Human-in-the-loop¶\n\nFirst, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See the how-to guides for examples.\n\n\n\n## Memory¶\n\nSecond, checkpointers allow for \"memory\" between interactions.  In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See Add memory for information on how to add and manage conversation memory using checkpointers.\n\n\n\n## Time Travel¶\n\nThird, checkpointers allow for \"time travel\", allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.\n\n\n\n## Fault-tolerance¶\n\nLastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.\n\n\n\n## Pending writes¶\n\nAdditionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.", "url": "https://langchain-ai.github.io/langgraph/concepts/persistence/", "path": "/langgraph/concepts/persistence/", "domain": "langchain-ai.github.io", "source": "LangGraph 공식문서", "content_length": 45566, "crawled_at": "2025-07-18 03:24:40"}
{"title": "Streaming¶", "content": "Get started\n\nGuides\n    \n  \n\n\n\n\n\n\n\n\n            Guides\n          \n\n\n\n\n\n    Agent development\n    \n  \n\n\n\n\n\n            Agent development\n          \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n\n    Run an agent\n    \n  \n\n\n\n\n\n\n\n\n\n    LangGraph APIs\n    \n  \n\n\n\n\n\n            LangGraph APIs\n          \n\n\n\n\n    Graph API\n    \n  \n\n\n\n\n\n\n    Functional API\n    \n  \n\n\n\n\n\n\n    Runtime\n    \n  \n\n\n\n\n\n\n\n\n\n    Core capabilities\n    \n  \n\n\n\n\n\n            Core capabilities\n          \n\n\n\n\n\n    Streaming\n    \n  \n\n\n\n\n\n            Streaming\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      What’s possible with LangGraph streaming\n    \n\n\n\n\n\n\n\n\n    Stream outputs\n    \n  \n\n\n\n\n\n    Use Server API\n    \n  \n\n\n\n\n\n\n\n\n    Persistence\n    \n  \n\n\n\n\n\n\n    Durable execution\n    \n  \n\n\n\n\n\n\n    Memory\n    \n  \n\n\n\n\n\n\n    Context\n    \n  \n\n\n\n\n\n\n    Models\n    \n  \n\n\n\n\n\n\n    Tools\n    \n  \n\n\n\n\n\n\n    Human-in-the-loop\n    \n  \n\n\n\n\n\n\n    Time travel\n    \n  \n\n\n\n\n\n\n    Subgraphs\n    \n  \n\n\n\n\n\n\n    Multi-agent\n    \n  \n\n\n\n\n\n\n    MCP\n    \n  \n\n\n\n\n\n\n    Tracing\n    \n  \n\n\n\n\n\n\n\n\n\n\n    Platform-only capabilities\n    \n  \n\n\n\n\n\n            Platform-only capabilities\n          \n\n\n\n\n    LangGraph Platform\n    \n  \n\n\n\n\n\n\n    Authentication & access control\n    \n  \n\n\n\n\n\n\n    Assistants\n    \n  \n\n\n\n\n\n\n    Double-texting\n    \n  \n\n\n\n\n\n\n    Webhooks\n    \n  \n\n\n\n\n\n\n    Cron jobs\n    \n  \n\n\n\n\n\n\n    Server customization\n    \n  \n\n\n\n\n\n\n    Data management\n    \n  \n\n\n\n\n\n\n    Deployment\n\nAgent development\n    \n  \n\n\n\n\n\n            Agent development\n          \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n\n    Run an agent\n\nOverview\n\nRun an agent\n\nLangGraph APIs\n    \n  \n\n\n\n\n\n            LangGraph APIs\n          \n\n\n\n\n    Graph API\n    \n  \n\n\n\n\n\n\n    Functional API\n    \n  \n\n\n\n\n\n\n    Runtime\n\nGraph API\n\nFunctional API\n\nRuntime\n\nCore capabilities\n    \n  \n\n\n\n\n\n            Core capabilities\n          \n\n\n\n\n\n    Streaming\n    \n  \n\n\n\n\n\n            Streaming\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      What’s possible with LangGraph streaming\n    \n\n\n\n\n\n\n\n\n    Stream outputs\n    \n  \n\n\n\n\n\n    Use Server API\n    \n  \n\n\n\n\n\n\n\n\n    Persistence\n    \n  \n\n\n\n\n\n\n    Durable execution\n    \n  \n\n\n\n\n\n\n    Memory\n    \n  \n\n\n\n\n\n\n    Context\n    \n  \n\n\n\n\n\n\n    Models\n    \n  \n\n\n\n\n\n\n    Tools\n    \n  \n\n\n\n\n\n\n    Human-in-the-loop\n    \n  \n\n\n\n\n\n\n    Time travel\n    \n  \n\n\n\n\n\n\n    Subgraphs\n    \n  \n\n\n\n\n\n\n    Multi-agent\n    \n  \n\n\n\n\n\n\n    MCP\n    \n  \n\n\n\n\n\n\n    Tracing\n\nStreaming\n    \n  \n\n\n\n\n\n            Streaming\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      What’s possible with LangGraph streaming\n    \n\n\n\n\n\n\n\n\n    Stream outputs\n    \n  \n\n\n\n\n\n    Use Server API\n\nOverview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      What’s possible with LangGraph streaming\n\nWhat’s possible with LangGraph streaming\n\nStream outputs\n\nUse Server API\n\nPersistence\n\nDurable execution\n\nMemory\n\nContext\n\nModels\n\nTools\n\nHuman-in-the-loop\n\nTime travel\n\nSubgraphs\n\nMulti-agent\n\nMCP\n\nTracing\n\nPlatform-only capabilities\n    \n  \n\n\n\n\n\n            Platform-only capabilities\n          \n\n\n\n\n    LangGraph Platform\n    \n  \n\n\n\n\n\n\n    Authentication & access control\n    \n  \n\n\n\n\n\n\n    Assistants\n    \n  \n\n\n\n\n\n\n    Double-texting\n    \n  \n\n\n\n\n\n\n    Webhooks\n    \n  \n\n\n\n\n\n\n    Cron jobs\n    \n  \n\n\n\n\n\n\n    Server customization\n    \n  \n\n\n\n\n\n\n    Data management\n    \n  \n\n\n\n\n\n\n    Deployment\n\nLangGraph Platform\n\nAuthentication & access control\n\nAssistants\n\nDouble-texting\n\nWebhooks\n\nCron jobs\n\nServer customization\n\nData management\n\nDeployment\n\nReference\n\nExamples\n\nAdditional resources\n\nWhat’s possible with LangGraph streaming\n\n\n\n## Streaming¶\n\nLangGraph implements a streaming system to surface real-time updates, allowing for responsive and transparent user experiences.\n\nLangGraph’s streaming system lets you surface live feedback from graph runs to your app.\nThere are three main categories of data you can stream:\n\nWorkflow progress — get state updates after each graph node is executed.\n\nLLM tokens — stream language model tokens as they’re generated.\n\nCustom updates — emit user-defined signals (e.g., “Fetched 10/100 records”).\n\n\n\n## What’s possible with LangGraph streaming¶\n\nStream LLM tokens — capture token streams from anywhere: inside nodes, subgraphs, or tools.\n\nEmit progress notifications from tools — send custom updates or progress signals directly from tool functions.\n\nStream from subgraphs — include outputs from both the parent graph and any nested subgraphs.\n\nUse any LLM — stream tokens from any LLM, even if it's not a LangChain model using the custom streaming mode.\n\n\n```\ncustom\n```\nUse multiple streaming modes — choose from values (full state), updates (state deltas), messages (LLM tokens + metadata), custom (arbitrary user data), or debug (detailed traces).\n\n\n```\nvalues\n```\n\n```\nupdates\n```\n\n```\nmessages\n```\n\n```\ncustom\n```\n\n```\ndebug\n```", "url": "https://langchain-ai.github.io/langgraph/concepts/streaming/", "path": "/langgraph/concepts/streaming/", "domain": "langchain-ai.github.io", "source": "LangGraph 공식문서", "content_length": 4927, "crawled_at": "2025-07-18 03:24:41"}
{"title": "Human-in-the-loop¶", "content": "Get started\n\nGuides\n    \n  \n\n\n\n\n\n\n\n\n            Guides\n          \n\n\n\n\n\n    Agent development\n    \n  \n\n\n\n\n\n            Agent development\n          \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n\n    Run an agent\n    \n  \n\n\n\n\n\n\n\n\n\n    LangGraph APIs\n    \n  \n\n\n\n\n\n            LangGraph APIs\n          \n\n\n\n\n    Graph API\n    \n  \n\n\n\n\n\n\n    Functional API\n    \n  \n\n\n\n\n\n\n    Runtime\n    \n  \n\n\n\n\n\n\n\n\n\n    Core capabilities\n    \n  \n\n\n\n\n\n            Core capabilities\n          \n\n\n\n\n    Streaming\n    \n  \n\n\n\n\n\n\n    Persistence\n    \n  \n\n\n\n\n\n\n    Durable execution\n    \n  \n\n\n\n\n\n\n    Memory\n    \n  \n\n\n\n\n\n\n    Context\n    \n  \n\n\n\n\n\n\n    Models\n    \n  \n\n\n\n\n\n\n    Tools\n    \n  \n\n\n\n\n\n\n\n    Human-in-the-loop\n    \n  \n\n\n\n\n\n            Human-in-the-loop\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Key capabilities\n    \n\n\n\n\n\n      Patterns\n    \n\n\n\n\n\n\n\n\n    Add human intervention\n    \n  \n\n\n\n\n\n    Use Server API\n    \n  \n\n\n\n\n\n\n\n\n    Time travel\n    \n  \n\n\n\n\n\n\n    Subgraphs\n    \n  \n\n\n\n\n\n\n    Multi-agent\n    \n  \n\n\n\n\n\n\n    MCP\n    \n  \n\n\n\n\n\n\n    Tracing\n    \n  \n\n\n\n\n\n\n\n\n\n\n    Platform-only capabilities\n    \n  \n\n\n\n\n\n            Platform-only capabilities\n          \n\n\n\n\n    LangGraph Platform\n    \n  \n\n\n\n\n\n\n    Authentication & access control\n    \n  \n\n\n\n\n\n\n    Assistants\n    \n  \n\n\n\n\n\n\n    Double-texting\n    \n  \n\n\n\n\n\n\n    Webhooks\n    \n  \n\n\n\n\n\n\n    Cron jobs\n    \n  \n\n\n\n\n\n\n    Server customization\n    \n  \n\n\n\n\n\n\n    Data management\n    \n  \n\n\n\n\n\n\n    Deployment\n\nAgent development\n    \n  \n\n\n\n\n\n            Agent development\n          \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n\n    Run an agent\n\nOverview\n\nRun an agent\n\nLangGraph APIs\n    \n  \n\n\n\n\n\n            LangGraph APIs\n          \n\n\n\n\n    Graph API\n    \n  \n\n\n\n\n\n\n    Functional API\n    \n  \n\n\n\n\n\n\n    Runtime\n\nGraph API\n\nFunctional API\n\nRuntime\n\nCore capabilities\n    \n  \n\n\n\n\n\n            Core capabilities\n          \n\n\n\n\n    Streaming\n    \n  \n\n\n\n\n\n\n    Persistence\n    \n  \n\n\n\n\n\n\n    Durable execution\n    \n  \n\n\n\n\n\n\n    Memory\n    \n  \n\n\n\n\n\n\n    Context\n    \n  \n\n\n\n\n\n\n    Models\n    \n  \n\n\n\n\n\n\n    Tools\n    \n  \n\n\n\n\n\n\n\n    Human-in-the-loop\n    \n  \n\n\n\n\n\n            Human-in-the-loop\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Key capabilities\n    \n\n\n\n\n\n      Patterns\n    \n\n\n\n\n\n\n\n\n    Add human intervention\n    \n  \n\n\n\n\n\n    Use Server API\n    \n  \n\n\n\n\n\n\n\n\n    Time travel\n    \n  \n\n\n\n\n\n\n    Subgraphs\n    \n  \n\n\n\n\n\n\n    Multi-agent\n    \n  \n\n\n\n\n\n\n    MCP\n    \n  \n\n\n\n\n\n\n    Tracing\n\nStreaming\n\nPersistence\n\nDurable execution\n\nMemory\n\nContext\n\nModels\n\nTools\n\nHuman-in-the-loop\n    \n  \n\n\n\n\n\n            Human-in-the-loop\n          \n\n\n\n\n\n    Overview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Key capabilities\n    \n\n\n\n\n\n      Patterns\n    \n\n\n\n\n\n\n\n\n    Add human intervention\n    \n  \n\n\n\n\n\n    Use Server API\n\nOverview\n    \n  \n\n\n\n\n    Overview\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Key capabilities\n    \n\n\n\n\n\n      Patterns\n\nKey capabilities\n\nPatterns\n\nAdd human intervention\n\nUse Server API\n\nTime travel\n\nSubgraphs\n\nMulti-agent\n\nMCP\n\nTracing\n\nPlatform-only capabilities\n    \n  \n\n\n\n\n\n            Platform-only capabilities\n          \n\n\n\n\n    LangGraph Platform\n    \n  \n\n\n\n\n\n\n    Authentication & access control\n    \n  \n\n\n\n\n\n\n    Assistants\n    \n  \n\n\n\n\n\n\n    Double-texting\n    \n  \n\n\n\n\n\n\n    Webhooks\n    \n  \n\n\n\n\n\n\n    Cron jobs\n    \n  \n\n\n\n\n\n\n    Server customization\n    \n  \n\n\n\n\n\n\n    Data management\n    \n  \n\n\n\n\n\n\n    Deployment\n\nLangGraph Platform\n\nAuthentication & access control\n\nAssistants\n\nDouble-texting\n\nWebhooks\n\nCron jobs\n\nServer customization\n\nData management\n\nDeployment\n\nReference\n\nExamples\n\nAdditional resources\n\nKey capabilities\n\nPatterns\n\n\n\n## Human-in-the-loop¶\n\nTo review, edit, and approve tool calls in an agent or workflow, use LangGraph's human-in-the-loop features to enable human intervention at any point in a workflow. This is especially useful in large language model (LLM)-driven applications where model output may require validation, correction, or additional context.\n\nTip\n\nFor information on how to use human-in-the-loop, see Enable human intervention and Human-in-the-loop using Server API.\n\n\n\n## Key capabilities¶\n\nPersistent execution state: Interrupts use LangGraph's persistence layer, which saves the graph state, to indefinitely pause graph execution until you resume. This is possible because LangGraph checkpoints the graph state after each step, which allows the system to persist execution context and later resume the workflow, continuing from where it left off. This supports asynchronous human review or input without time constraints.\nThere are two ways to pause a graph:\n\nDynamic interrupts: Use interrupt to pause a graph from inside a specific node, based on the current state of the graph.\nStatic interrupts: Use interrupt_before and interrupt_after to pause the graph at defined points, either before or after a node executes.\n\n\n\nAn example graph consisting of 3 sequential steps with a breakpoint before step_3.\n\nPersistent execution state: Interrupts use LangGraph's persistence layer, which saves the graph state, to indefinitely pause graph execution until you resume. This is possible because LangGraph checkpoints the graph state after each step, which allows the system to persist execution context and later resume the workflow, continuing from where it left off. This supports asynchronous human review or input without time constraints.\n\nThere are two ways to pause a graph:\n\nDynamic interrupts: Use interrupt to pause a graph from inside a specific node, based on the current state of the graph.\n\n\n```\ninterrupt\n```\nStatic interrupts: Use interrupt_before and interrupt_after to pause the graph at defined points, either before or after a node executes.\n\n\n```\ninterrupt_before\n```\n\n```\ninterrupt_after\n```\nAn example graph consisting of 3 sequential steps with a breakpoint before step_3.\n\nFlexible integration points: Human-in-the-loop logic can be introduced at any point in the workflow. This allows targeted human involvement, such as approving API calls, correcting outputs, or guiding conversations.\n\nFlexible integration points: Human-in-the-loop logic can be introduced at any point in the workflow. This allows targeted human involvement, such as approving API calls, correcting outputs, or guiding conversations.\n\n\n\n## Patterns¶\n\nThere are four typical design patterns that you can implement using interrupt and Command:\n\n\n```\ninterrupt\n```\n\n```\nCommand\n```\nApprove or reject: Pause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action. This pattern often involves routing the graph based on the human's input.\n\nEdit graph state: Pause the graph to review and edit the graph state. This is useful for correcting mistakes or updating the state with additional information. This pattern often involves updating the state with the human's input.\n\nReview tool calls: Pause the graph to review and edit tool calls requested by the LLM before tool execution.\n\nValidate human input: Pause the graph to validate human input before proceeding with the next step.", "url": "https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/", "path": "/langgraph/concepts/human_in_the_loop/", "domain": "langchain-ai.github.io", "source": "LangGraph 공식문서", "content_length": 7243, "crawled_at": "2025-07-18 03:24:42"}
